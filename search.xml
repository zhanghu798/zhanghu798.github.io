<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[循环神经网络]]></title>
    <url>%2F2017%2F11%2F19%2Frnn%2F</url>
    <content type="text"><![CDATA[Recurrent Neural Network 本文以下RNN特指循环神经网络 深度神经网络极大的提高了浅层网络的模型拟合数据的能力 CNN的出现引入了“感受野”的同时共享参数的机制大大减少DNN的网络参数 而RNN的出现则是用“记忆单元”可以使得有时序的数据随着时间改变记忆，这个记忆贯穿整个时间流，从而有了时序处理的能力 1. RNN及变种 1.1. RNN Recurrent Neural Networks Tutorial RNN结构及按时序展开图 网络结构的公式 \[\left . \begin{aligned} &amp; \text{h}_t = f \big(\text{x}_t \text{U} + \text{h}_{t-1} \text{W} + b_h \big ) \\ &amp; \hat{\text{y}_t} = \text{softmax}( \text{h}_t \text{V} + b_y) \end{aligned} \right. \tag{1} \] 参数说明： 参数 说明 \(\text{x}_t\) 第t时刻的输入。假设：\(1 \times n_x\)。 \(n_x\)：一般指输入数据纬度， 如文本问题中，embedding词向量长度大小或one-hot中的长度 \(\text{h}_t\) 第t时刻的记忆。假设：\(1 \times n_h\)。 \(n_h\)：隐层的维度 \(f\) 非线形激励。如：tanh \(\text{U}\) 输入系数矩阵, \(n_x \times n_h\) \(\text{W}\) 记忆系数矩阵, \(n_h \times n_h\) \(\text{V}\) 输出系数矩阵, \(n_h \times \text{vocab_size} 。\text{vocab_size}\)：非限定 $_t $ 第t时刻的输出, \(1 \times \text{vocab_size}。\text{vocab_size}\): 词库大小 \(b_h\) 偏置，\(1 \times n_h\)。更确切的说，这个是属于softmax层的参数 \(b_y\) 偏置, \(1 \times \text{vocab_size} 。\text{vocab_size}\)：预测词库大小，非限定。softmax层参数 对于RNNLM： 损失函数： 对于第\(t\)时刻的one-hot形式的标准结果\(h_t\), 使用交叉熵 \[ \text{loss }_t = \text{-y}_t \log \hat{\text{y}}_t \tag{2} \] 整体loss \[ \text{Loss} = \sum_{t}^{T} \text{loss}_t \tag{3} \] 2. BPTT RNN的训练方法： Backpropagation Through Time BPTT示意图 \[ \begin{aligned} \frac{\partial{\text{Loss}}}{\partial{W}} = &amp; \sum_{k=0}^{T} \frac{\partial{\text{loss}_t}}{\partial{\text{W}_{k}}} \\ = &amp; \sum_{k=0}^{T} \frac{\partial{\text{loss}_t}}{\partial{\hat{\text{y}}_t}} \ \frac{\partial{\hat{\text{y}}_t}}{\partial{\hat{\text{h}}_{t}}} \ \frac{\partial{\hat{\text{h}}_t}}{\partial{\hat{\text{h}}_{k}}} \ \frac{\partial{\hat{\text{h}}_{k}}}{\partial{\text{W}_{k}}} \\ = &amp; \sum_{k=0}^{T} \ \frac{\partial{L_T}}{\partial{\hat{\text{y}}_t}} \ \frac{\partial{\hat{\text{y}}_t}}{\partial{\hat{\text{h}}_{t}}} \ \Bigg(\prod_{j=k+1}^{t} \frac{\partial{\hat{\text{h}}_j}}{\partial{\hat{\text{h}}_{j-1}}} \Bigg) \ \frac{\partial{\hat{\text{h}}_{k}}}{\partial{\text{W}_{k}}} \end{aligned} \tag{4} \] 特别说明： \(\text{W}\)是整个过程中共享的 上式中\({\text{W}_{k}}\)特指在地\(k\)时刻的对应的\(\text{W}\), 表示第\(k\)时刻的隐藏（记忆）状态\(\text{h}_{k}\)是由\({\text{W}_{k}}\)得到的， 即，在上式中，\(\text{W}\)看做是\(k\)个独立\({\text{W}_{k}}\)，只是具有相同的值\(\text{W}\) \[\frac{\partial{\hat{\text{h}}_j}}{\partial{\hat{\text{h}}_{j-1}}} = \frac{\partial{\hat{f}_j}}{\partial{\text{v}_{f_{in}}}} W \tag{5}\] 将式（5）代入（4）导致RNN做BP时，在长时序列数据训练时容易导致梯度爆炸或远处时刻的损失反映不到参数的梯度上（相对梯度消失）。 对于梯度爆炸，可以通过梯度裁剪的方法使得问题被避免： \[\boldsymbol{g} = \min\left(\frac{\theta}{\|\boldsymbol{g}\|}, 1\right)\boldsymbol{g} \tag{6}\] 这种形式类似BN层，都是把异常数据强制拉回到正常状态（BN层是将参数分布拉到正态分布，梯度剪裁是强制将过大的梯度设置为1） 2.1. LSTM 1997 Long Short Term Memory LSTM通过引入“遗忘门”及“记忆叠加门”使得记忆更加灵活，而选择记忆+叠加新记忆更新为新的记忆状态的形式也使得LSTM具有相比RNN的梯度消失和梯度爆炸问题得到较大的缓解，从而使得LSTM可以处理较长时序问题 2.1.1. 内部结构示意图 单层LSTM内部结构示意图 2.1.2. LSTM内部示意图及计算方法 忘记门 输入和输入门 更新cell状态 输出门 \[ \left . \begin{aligned} &amp;f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) &amp; &amp; 忘记门，忘记系数（选择保留的系数），决定的丢弃记忆\\ &amp;i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) &amp; &amp; 新的记忆选择系数\\ &amp; \widetilde{C}_t = \text{tanh}\big(W_C \cdot [h_{t-1}, x_t] + b_c\big) &amp; &amp; 新的记忆 ，决定增强的记忆 \\ &amp;C_t = f_t \odot C_{t-1} + i_t \odot \widetilde{C}_t &amp; &amp; 更新记忆：t-1时刻状态 \times 选择保留系数 + 新记忆 \times 新记忆系数\\ &amp; o_t = \sigma \big(W_O[h_{t-1}, x_t] + b_O\big) &amp; &amp; 上一时刻的隐层状态和本次输入决定本次输出\\ &amp; h_t= o_t \cdot \text{tanh} \big(C_t\big) &amp; &amp; 前一时刻输出和当前网络状态决定当前t时刻隐层状态 \\ \end{aligned} \right. \] 2.2. GRU 2014 gru内部结构示意图 2.3. RNN中的DropOut 将输入向量各个维度随机进行Drop操作，而达到增强泛化的作用， 详见http://reset.pub/2017/05/19/dnn/#more Bagging 有性繁殖 VS 无性繁殖 共享参数 以p为dropout概率，则 1234is_drop = True if random(0, 1) &lt; p else Falseif x_i not is_drop: x_i = x_i / (1 - p) 3. Seq2Seq seq2seq基本示意图 encoder-decoder基本示意图 3.1. Beam-search 用于最大路径概率预测， 主要是降低时间复杂度。通常是计算时序问题的最大概率路径问题。如HMM，CRF中给定模型参数，求最大概率路径问题。是动态规划每次截断第t-1的前K大路径，传播到第t时刻。 Beam-search是一种介于viterbi和贪心算法见的算法。是最优解和最快效率的一个平衡 3.1.1. Beam-search示意图 beam-search 3.1.2. 求最大概率路径的算法对比 假设每一步都有n个状态，一共有s个时间步 算法 时间复杂度 是否最优 穷举 \(n^s\) 是 viterbi \(s \times n^2\) 是 贪心 \(s \times n \times k\), 即Beam search中的\(k=1\)的情况 否 Beam search \(s \times n \times k\)。\(1 \leq k \leq n\)。 当\(k=1\)时为贪心算法; 当\(k=n\)时，为viterbi算法 否 3.2. Attention https://github.com/mli/gluon-tutorials-zh/blob/master/chapter_natural-language-processing/nmt.md attention主要解决信息过剩（RNN时间步较长后梯度消失或爆炸）的情况下，通常需要关注某些主要因素时，进行注意力分配的问题 attention的本质是对历史状态加权作为输入的一部分，达到注意力的目的 即如下式1中\(C_t\)是注意力部分 注意力计算部分 \[C_t = \overline{h}_s \odot a_t \tag{9}\] 注意力计算如下式，其中\(\overline{h}_s\)是历史数据向量组成的向量，\(a_t\)对应的权值 注意力和当前状态合并作为输入进行预测 \[\tilde{h}_t = \tanh(W_c[C_t; h_t]) \tag{8}\] 3.2.1. global-attention(soft attention) attention-global基本示意图 归一化的权重计算 权重计算 3.2.2. local-attention(hard attention) 详见论文 Effective Approaches to Attention-based Neural Machine Translation attion-local基本示意图 权重计算 核心位置计算 3.3. seq2seq中的其他trick unk replace reverse feed input feed-input 4. 参考 Incorporating Copying Mechanism in Sequence-to-Sequence Learning Recurrent Neural Networks Tutorial Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation Effective Approaches to Attention-based Neural Machine Translation Understanding LSTM Networks [译] 理解 LSTM 网络 Sequence-to-Sequence Learning as Beam-Search Optimization Sequence to Sequence Learning with Neural Networks Attention Is All You Need Recurrent Neural Network Regularization How to Use Dropout with LSTM Networks for Time Series Forecasting An Empirical Exploration of Recurrent Network Architectures Deep Learning for Natural Language Processing : Hang Li]]></content>
      <categories>
        <category>机器学习</category>
        <category>DL</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>Seq2Seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络简介]]></title>
    <url>%2F2017%2F05%2F23%2Fcnn%2F</url>
    <content type="text"><![CDATA[Convolutional Neural Networks 卷积神经网络简介 “http://cs231n.github.io/convolutional-networks/#overview” 动图 1.卷积层卷积过程的动态示意图 图片中“Output”是指http://reset.pub/2017/05/19/dnn/中\(V_{IN}\)，即激励层的输入数据 \(\boldsymbol{W} \cdot \boldsymbol{X} + \boldsymbol{b}\) 本文以图片问题中的CNN为例 1. CNN基本思想 CNN是DNN的一种特殊形式 考虑使用DNN做图像分类问题，可以将图片的每个像素上每个通道上的数据看成是一个神经元 假设对于\(n \times a\)大小的图片，每个像素点有RGB共3个颜色通道，则数据输入层为\(3 \times a \times a\) 假设有M层隐层，每层有N个神经元则一共有\(3 a^2 N^M + MN\)个参数。参数量较大。 针对参数量庞大问题有了卷积神经网络，如以上动态图所示，同一个卷积窗口在遍历一张图片时，参数是共享的 以上就CNN中最重要的思想：卷积核及共享权值 2. CNN结构基本框架 图2，CNN框架示意图 INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]M -&gt; [FC -&gt; RELU]K -&gt; FC 其中： CONV：卷积层 RELU：代表激励层 POOL：池化层 FC：全链接层 3. CNN结构说明 3.1. 数据输入层 图片中每个颜色通道代表一个维度的数据，每个通道上为一张含有位置信息的二维表 3.2. 卷积层 卷积层主要作用是降维 另一种理解是，图片信息中相连区域内的像素点比较相关，通过选定位置相关的区域进行处理可以提高网络的泛化能力 3.2.1. 滑动窗口 卷积窗口尺寸：f 卷积窗口滑动间隔：s 卷积深度：d 3.2.2. 数据对齐 对于输入图像为\(a\)的正方形图片， 使用宽度为\(f\)的卷积窗口， 以步长为\(s\)的方式滑动。一共可以得到 \(n^2\)个数据，其中： \[n = \frac{a-f}{s} + 1 \tag{1}\] 对于最后一个滑动窗口可能不能对齐是\(n\)为非正数，可以通过调整卷积窗口的尺寸和步长来实现对齐 另一个解决滑动不完美的方法是在周围填充0。 每行最少填充个数 \[q=f+(\lceil n \rceil-1)s - a \tag{2}\] 其中\(\lceil n \rceil\)为\(n\)的向上取整 则经过调整后 \[n = \frac{a + q -f}{s} + 1 \tag{3}\] 另外http://cs231n.github.io/convolutional-networks/#overview中“no zero-padding”的P是指填充圈数，则\(q\)与\(P\)的关系为 \[P=\frac{1}{2}q \tag{4}\] 在保证卷机核能卷积操作与数据尺寸对齐的情况下，可以继续使用0填充来扩充数据边界使的原来数据边界作为数据中心达到边界中心化的目的，对于已经通过调整超参数或0填充的方法使的数据可以滑动对齐的长度为\(a\)的矩阵数据，在固定超参数\(f\)和\(s\)的情况下，有效填充个数\(q\)满足如下： \[ \left\{ \begin{aligned} &amp; 0 \leqslant q \leqslant 2(f-1) \\ &amp; (q+a-f) \% s = 0 \end{aligned} \right. \tag{5} \] ‘\(\%\)’：取余符号 3.2.3. 卷积过程及计算 全部过程如动图（1）所示 针对每一个固定卷积窗口，按照超参数左右，上下方向\(s\)移动，最终得到\(n^2\)（式（1））组数据。针对某颜色通道上\(i\)上，对于给定的参数\(\boldsymbol{W_i}\)，假设滑动窗口内的数据为\(\boldsymbol{X_i}\)，在feture map中相应位置的值为\(\boldsymbol{W_i}\cdot \boldsymbol{X_i}\)，综合考虑此次滑动是的对应输出为 \[O_{j, k} = \sum_{i=1}^{d} \cdot \boldsymbol{W_i}\cdot \boldsymbol{X_i} \tag{6}\] 其中\(d\)为输出feture map中二维表的个数为\(d\)，一般指原始输入的颜色通道个数3，或者是前一层卷积单元的个数（前一层卷积的深度） 对于输入数数据维度\(\boldsymbol{X}\)的大小为\((K, a, a)\)，其对应的下标分别为 \(k\)，\(i\)，\(j\)。 如果是数据输入层，\(K\)为颜色通道个数3。这个表示方法和动图（1）中的表示方法有点不一样，主要是考虑高纬度矩阵表示习惯，将最高维放在第一维 卷积窗口的权重矩阵\(\boldsymbol{W}\)的大小为\((K, f, f)\), 同一卷积层有\(m\)个filter，则卷积过后形成的feture map大小为\(m \times n \times n\) 3.3. pooling层 图3，max pooling示意图 池化层，通过采样的方式对数据进行降维。pool是给定观察窗口大小和滑动长度，每次滑动一个位置，对窗口选择的数据进行一次下采样： Max pooling 求二维表中的最大值 Average pooling 求二维表中的平均 3.4. BN层 BN层，http://reset.pub/2017/05/19/dnn/ 3.5. 全链接层 层级之间的链接是完全的，同DNN 作用：增强特征表达能力，即普通深度神经网络的最大优势 4. CNN的反向传播 http://reset.pub/2017/05/19/dnn/ 可以将动图（1）展开成传统DNN的结构，由于共享权值，为了方便推导，将权值看成输入神经元，输入看成链接权值可以很好结合dnn中式（13）可以得到， 4.1. 数据层+卷积层+激励层的反向传播 卷积深度超过1的情况下每个输入数据要考虑所有深度的情况，所以每个输入数据的导数递推公式和下一层的数据处理方式有关。 以下讨论卷积深度为1的情况 \[ \begin{aligned} &amp; \frac{\partial{\ell}}{\partial X_{kk\ ,\ i\ ,\ j_{i}}} = \sum_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}} \ f^\prime \big(V_{IN_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}} \ \ \ \big) \ \cdot \ W_{kk\ ,\ s_{i}\ ,\ t_{j}} \ \cdot \ \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}} \end{aligned} \tag{7} \] \[ \begin{aligned} &amp; \frac{\partial{\ell}}{\partial W_{kk\ ,\ s\ ,\ t}} = \sum_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}} \ f^\prime \big(V_{IN_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}} \ \ \ \big) \ \cdot \ X_{kk\ ,\ i_{s} \ \ , \ i_{t}} \ \cdot \ \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}} \end{aligned} \tag{8} \] \[ V_{IN_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}} = \sum_{kk=1}^{k}V_{IN_{kk\ ,\ i+1\ ,\ j_{i+1}}} \ \ = \sum_{kk=1}^{k} \Big( \boldsymbol{W}_{kk} \cdot \boldsymbol{X}_{kk\ ,\ area=i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}} + b \Big) \tag{9} \] \[ O_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}} = f\Big(V_{IN_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}}\ \ \Big) \tag{10} \] 参数说明 \(\ell\): 损失函数 \(k\)上一层的卷积窗口个数或图片数据中通道数据的个数3 \(kk\)：\(k\)的索引 \(O_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}\): 数据层的第\(kk\)层的第\(i\)行\(j_{i}\)列与卷积层的第\(kk\)维的第\(s_{i}\)行，第\(s_{j}\)列对齐时内积经过激励函数的后的输出 \(f\)：激励函数 \(\boldsymbol{W}_{kk}\): 第\(kk\)维的二维权重矩阵 \(\boldsymbol{X}_{kk\ ,\ area=i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}\): 当卷积窗口滑动时选中数据窗口中\((i, j_{i}位置\)与卷积窗口\(s_{i}\ ,\ s_{j}\)的位置重合时所选区域在第\(kk\)维的二维数据矩阵 4.1.1. 数据层(上层结构输出)++Max pooling反向传播 \[ \begin{aligned} &amp; \frac{\partial{O_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}}}{\partial X_{kk\ ,\ i\ ,\ j_{i}}} \\ = &amp; \Big(X_{kk\ ,\ i\ ,\ j_{i}} == max \big(area_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}\big) \Big) \\ = &amp; \Big(X_{kk\ ,\ i\ ,\ j_{i}} == O_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}} \Big) \end{aligned} \tag{11} \] 对于没次pooling层选中 4.1.2. 数据层(上层结构输出)+Average pooling反向传播 假设\(n\)为卷积窗口的宽度 \[ \begin{aligned} &amp; \frac{\partial{O_{i\ ,\ j_{i} \ \ \leftrightarrow \ \ s_{i}\ ,\ s_{j}}}}{\partial X_{kk\ ,\ i\ ,\ j_{i}}} = \frac{1}{n^2} \end{aligned} \tag{12} \] 5. 经典CNN模型 说明见：http://cs231n.github.io/convolutional-networks/#norm 及相关论文 LeNet: 1990， http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf AlexNet: 2012， http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks ZF Net: 2013， https://arxiv.org/abs/1311.2901 GoogLeNet: 2014， https://arxiv.org/abs/1409.4842 VGGNet: 2014， http://www.robots.ox.ac.uk/~vgg/research/very_deep/ ResNet: 2015，残差网络，https://arxiv.org/abs/1512.03385 6. 参考 [1] http://cs231n.github.io/convolutional-networks/#overview [2] 2017.03.15《Deep Learning翻译》https://exacity.github.io/deeplearningbook-chinese/ [3] 2015.03.02 Google Inc Batch Normalization [4] CNN浅析和历年ImageNet冠军模型解析]]></content>
      <categories>
        <category>机器学习</category>
        <category>DL</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度神经网络简介]]></title>
    <url>%2F2017%2F05%2F19%2Fdnn%2F</url>
    <content type="text"><![CDATA[Deep Neural Networks 1. 人工神经网络介绍 https://zh.wikipedia.org/wiki/人工神经网络 2. DNN与传统机器学习方法对比 优点 降低或避免对特征工程的依赖，深度学习能做特征工程就是因为深度或广度，再加上非线性激励使得更多的组合特征出现，用损失函数来约束整个网络的发展方向，结果就是选择出来的比较容易过拟合的特征。但是这些特征没有什么可解释性，跟多是黑盒 缺点 黑盒模型 需要大样本支撑 对硬件要求高 3. DNN基本结构 输入层，隐藏层，隐藏层，… ， 输出层 图1，神经网络结构示意图 图2，神经元（感知器）示意图 4. 激励函数 激励函数的作用，增加非线形特征映射。激励函数的对比见：Must Know Tips/Tricks in Deep Neural Networks 4.1. Sigmoid函数 \[f(x) = \frac{1}{1+e^{-x}}\] 图3，sigmoid(x) 4.2. tanh函数 \[ tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}} \] 图4，tanh(x) 4.3. Relu \[f(x) = \max(0, x)\] 图5，relu(x) 4.4. Leaky ReLU \[ f(x) = \left\{ \begin{aligned} x，&amp; x \geqslant 0 \\ \alpha x， &amp; x &lt; 0 \\ \end{aligned} \right. \] 图6，leaky-relu(x) 更多激励函数见wiki 5. 神经网络训练方法 小批量梯度下降（Mini-batch Stochastic Gradient Descent） 小批量随机梯度下降较全量样本的梯度下降特点： - 收敛速度快，收随机梯度下降只是用少量的样本即可完成一次迭代 - 可以跳出局部最优解，找到更接近的全局最优的解，这大概是DNN随机梯度下降而不是全量梯度下降的最直接原因 5.1. 参数训练流程 1，定义损失函数，初始化参数： 2，正向传播得到损失 3，反向传播更新参数 4，判断是停止，否：则跳转到第2步 5.2. 反向传播算法 Backpropagation，BP 由最后一层逐层向前更新参数的算法 5.2.1. 剃度计算及BP推导 以全链接方式（图一）为结果，图二的方式的神经元链接方式为例： 符号说明： \(W_{i, j, k}\)：第\(i\)层神经元的第\(j\)个神经元与第\(i+1\)层的第\(k\)个神经元的链接权重 \(O_{i, j}\)：第\(i\)层神经元的第\(j\)个神经元的数学表达式 \[ O_{i\ \ ,\ \ j_{i}} = \left\{ \begin{aligned} &amp; f\Bigg(\sum_{j_{i-1}\ \ =1}^{J_{i-1}} W_{i-1\ \ , \ \ j_{i-1}\ \ ,\ \ j_{i}} \cdot O_{i-1\ \ , \ \ j_{i-1}} + b_{i\ \ ,\ \ j_{i}} \Bigg) \ \ \ &amp; &amp; 当O_{i\ ,\ \ j_{i}}为神经元 \\ &amp; x_{i\ ,\ j_{i}} &amp; &amp; 当O_{i\ ,\ j_{i}}为原始输入时 \end{aligned} \right. \tag{1} \] \(J_{i}\)： 第\(i\)层神经元个数 \(IN_{i,\ j_i}\)： 第\(i\)层的第\(j_i\)个神经元激励函数输入值的数学表达式，图2中激励函数f输入部分的数学表达式 \[ IN_{i,\ j_i} = \sum_{j_{i-1}\ \ =1}^{J_{i-1}} W_{i-1\ \ , \ \ j_{i-1}\ \ ,\ \ j_{i}} \cdot O_{i-1\ ,\ j-1} + b_{i\ ,\ j_{i}} \tag{2} \] \(V_{IN_{i,\ j_i}}\)：第\(i\)层的第\(j_i\)个神经元的\(IN_{i,\ j_i}\)的值，图2激励函数的输入部分数值 \(f^\prime \big(V_{IN_{i,\ j_i}} \ \big)\)：第\(i\)层的第\(j_i\)个神经元的激励函数（或损失函数）导数在\(V_{IN_{i,\ j_i}}\)处的数值 考虑第\(i\)层的第\(j\)个神经元与\(i+1\)层的第\(k\)神经元之间的链接权重\(W_{i, j, k}\)的，气候其后面链接神经元的计算表达式中含有\(W_{i, j, k}\)项的是：第\(i+1\)层第\(k\)个神经元，第\(i+2\)层到到\(n\)层的所有神经元 \[ \begin{aligned} &amp;\frac{\partial{O_{n,\ j_{n}} }}{\partial W_{i,\ j,\ k}} \\ = &amp; \left.\frac {\partial f(x)} {\partial x } \right|_{x=V_{IN_{n\ ,\ j_{n}}}} \ \cdot \ \frac {\partial \Big( b_{n, x} + \ \sum_{j_{n-1}\ =1}^{J_{n-1}} O_{n-1, \ j_{n-1}} \ \cdot \ W_{n-1,\ j_{n-1}\ \ ,\ j_{n}} \ \Big) } {\partial W_{i,\ j,\ k}} \\ = &amp; f^\prime \big(V_{IN_{n,\ j_{n}}} \ \big) \ \cdot \ \sum_{j_{n-1}\ \ =1}^{J_{n-1}} W_{n-1,\ j_{n-1}\ \ , \ j_{n}} \ \cdot \ \frac{\partial{O_{n-1,\ j_{n-1}} }}{\partial W_{i,\ j,\ k}} \\ \end{aligned} \tag{3} \] 直接推导（剃度的通项公式）： \[ \begin{aligned} \frac{\partial{O_{n\ ,\ j_{n}} }}{\partial W_{i\ ,\ j\ ,\ k}} = &amp; f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \big) \ \cdot \ \sum_{j_{n-1}\ \ =1}^{J_{n-1}} W_{n-1\ ,\ j_{n-1}\ , \ x} \ \cdot \ \frac{\partial{O_{n-1\ ,\ j_{n-1}} }}{\partial W_{i\ ,\ j\ ,\ k}} \\ = &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}} f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \big) \ \cdot \ W_{n-1\ ,\ j_{n-1}\ \ , \ j_{n}} \ \cdot \ \frac{\partial{O_{n-1\ ,\ j_{n-1}} }}{\partial W_{i\ ,\ j\ \ ,\ k}} \\ = &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}} f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \ \ \big) \ \cdot \ W_{n-1\ ,\ j_{n-1}\ \ , \ j_{n}} \ \cdot \ \Bigg( \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} f^\prime \big(V_{IN_{n-1,\ j_{n-1}}} \ \ \ \big) \ \cdot \ W_{n-2\ ,\ j_{n-2}\ \ , \ j_{n-1}} \ \cdot \ \frac{\partial{O_{n-2\ ,\ j_{n-2}} }}{\partial W_{i\ ,\ j\ \ ,\ k}} \Bigg) \\ = &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}} \ \ \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} \ \cdot\ f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \ \ \big) \ \cdot \ W_{n-1\ ,\ j_{n-1}\ \ , \ j_{n}} \ \cdot \ f^\prime \big(V_{IN_{n-1,\ j_{n-1}}} \ \ \ \big) \ \cdot \ W_{n-2\ ,\ j_{n-2}\ \ , \ j_{n-1}} \ \cdot \ \frac{\partial{O_{n-2\ ,\ j_{n-2}} }}{\partial W_{i\ ,\ j\ \ ,\ k}} \\ = &amp; \dots \\ = &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}}\ \ \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} \dots \sum_{j_{i+2}\ \ = \ 1}^{J_{i+2}} \ \cdot\ f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \ \ \big) \\ &amp; \cdot \Big(W_{n-1\ ,\ j_{n-1} \ \ , \ j_{n}} \ \cdot\ f^\prime \big(V_{IN_{n-1\ ,\ j_{n-1}}} \ \ \ \big) \Big) \\ &amp; \cdot \Big(W_{n-2\ ,\ j_{n-2} \ \ , \ j_{n-1}} \ \cdot\ f^\prime \big(V_{IN_{n-2\ ,\ j_{n-2}}} \ \ \ \big) \Big) \\ &amp;\cdot \dots \cdot\ \Big(W_{x\ ,\ j_{x} \ \ , \ j_{x+1}} \cdot f^\prime \big(V_{IN_{x\ ,\ j_{x}}} \ \ \ \big) \Big) \cdot \dots \cdot\ \\ &amp; \cdot\ \Big( W_{i+2\ ,\ j_{i+2} \ \ , \ j_{i+3}} \cdot f^\prime \big(V_{IN_{i+2\ ,\ j_{i+2}}} \ \ \ \big) \Big) \\ &amp; \cdot \ \Big( W_{i+1\ ,\ k \ \ , \ j_{i+2}} \cdot \ \frac{\partial{O_{i+1\ ,\ k} }}{\partial W_{i\ ,\ j\ ,\ k}} \Big) \\ = &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}}\ \ \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} \dots \sum_{j_{i+2}\ \ = \ 1}^{J_{i+2}} \ \cdot\ f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \ \ \big) \\ &amp; \cdot \prod_{x={i+2}}^{n-1}\Big(W_{x\ ,\ j_{x} \ \ , \ j_{x+1}} \cdot f^\prime \big(V_{IN_{x\ ,\ j_{x}}} \ \ \ \big) \Big) \\ &amp; \cdot \ \Big( W_{i+1\ ,\ k \ \ , \ j_{i+2}} \cdot \ \frac{\partial{O_{i+1\ ,\ k} }}{\partial W_{i\ ,\ j\ ,\ k}} \Big) \end{aligned} \tag{4} \] BP算法推导（剃度的由后往前的递推公式）： \[ \begin{aligned} \frac{\partial{O_{n\ ,\ j_{n}} }}{\partial O_{i+1\ ,\ j_{i+1}}} = &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}}\ \ \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} \dots \sum_{j_{i+2}\ \ = \ 1}^{J_{i+2}} \ \cdot\ f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \ \ \big) \\ &amp; \cdot \prod_{x={i+2}}^{n-1}\Big(W_{x\ ,\ j_{x} \ \ , \ j_{x+1}} \cdot f^\prime \big(V_{IN_{x\ ,\ j_{x}}} \ \ \ \big) \Big) \\ &amp; \cdot \ \Big( W_{i+1\ ,\ j_{i+1} \ \ , \ j_{i+2}} \cdot \ 1 \Big) \end{aligned} \tag{5} \] \[ \begin{aligned} \frac{\partial{O_{n\ ,\ j_{n}} }}{\partial O_{i\ ,\ j_{i}}} = &amp; \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ \cdot \ \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}} \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} \dots \sum_{j_{i+2}\ \ = \ 1}^{J_{i+2}} \ \cdot\ f^\prime \big(V_{IN_{n\ ,\ k}} \ \big) \\ &amp; \cdot \prod_{x={i+2}}^{n-1}\Big(W_{x\ ,\ j_{x} \ \ , \ j_{x+1}} \cdot f^\prime \big(V_{IN_{x\ ,\ j_{x}}} \ \ \ \big) \Big) \\ &amp; \cdot \Big(W_{i+1\ ,\ j_{i+1} \ \ , \ j_{x+2}} \cdot f^\prime \big(V_{IN_{i+1\ ,\ j_{i+1}}} \ \ \ \big) \Big) \\ &amp; \cdot \ \Big( W_{i\ ,\ j_{i} \ \ , \ j_{i+1}} \Big) \end{aligned} \tag{6} \] 对比式（5），式（6）得： \[ \begin{equation} \boxed{ \frac{\partial{O_{n\ ,\ k} }}{\partial O_{i\ ,\ j_{i}}} = \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ f^\prime \big(V_{IN_{i+1\ ,\ j_{i+1}}} \ \ \ \ \big) \ \cdot \ W_{i\ ,\ j_{i} \ \ , \ j_{i+1}} \ \cdot \ \frac{\partial{O_{n\ ,\ k} }}{\partial O_{i+1\ ,\ j_{i+1}}} } \ \ \ \ （其中i + 1 \leqslant n） \end{equation} \tag{7} \] 5.2.2. 使用BP + Mini-batch SGD训练的流程 假设损失函数为\(\ell = (y_i - \hat{y})^2\)， 第n层输出层，则式（7）中的第n层第k个神经元可以看作是计算损失的单元，即： \[ O_{n, k}=(y_i - \hat{y})^2 \tag{8} \] \[ V_{IN_{\ell}} = V(O_{n, k}) \tag{9} \] \[ \ell^\prime \big(V_{IN_{\ell}} \ \big) = 2V_{IN_{\ell}} \tag{10} \] 针对最后一层隐层到输出层的梯度为： \[ \frac{\partial{\ell}}{\partial W_{n\ ,\ j_{n}}} = \ell^\prime \big(V_{IN_{\ell}} \ \big) \cdot \ V_{O_{n\ ,\ j_{n} }} \ \ \ \ j_{n} = 1, 2, \dots , J_{n} \tag{11} \] \[ \frac{\partial{\ell}}{\partial O_{n\ ,\ j_{n}}} = \ell^\prime \big(V_{IN_{\ell}} \ \big) \cdot \ W_{n\ ,\ j_{n} } \ \ \ \ j_{n} = 1, 2, \dots , J_{n} \tag{12} \] 则，针对任意层的神经元反向传播梯度为： \[ \begin{aligned} &amp; \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} = \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ f^\prime \big(V_{IN_{i+1\ ,\ j_{i+1}}} \ \ \ \big) \ \cdot \ W_{i\ ,\ j_{i} \ \ , \ j_{i+1}} \ \cdot \ \frac{\partial{\ell}}{\partial O_{i+1\ ,\ j_{i+1}}} \end{aligned} \tag{13} \] 任意层权值\(w\)的反向传播梯度为： \[ \begin{aligned} \frac{\partial{\ell}}{\partial W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}} = &amp; \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} \ \cdot \ \frac{\partial O_{i\ ,\ j_{i}}} {\partial W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}} \\ = &amp; \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} \ \cdot \ f^\prime \big(V_{IN_{i\ ,\ j_{i}}} \ \big) \ \cdot \ \frac{\partial V_{IN_{i\ ,\ j_{i}}} } {\partial W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}} \\ = &amp; \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} \ \cdot \ f^\prime \big(V_{IN_{i\ ,\ j_{i}}} \ \big) \ \cdot \ V_{O_{i-1\ \ ,\ j_{i-1}}} \end{aligned} \tag{14} \] 参数范围： \[ \left\{ \begin{aligned} &amp; i = 2, 3, \dots, n-1 \\ &amp; j_i = 1, 2, 3, \dots, J_i \\ &amp; j_{i+1} = 1, 2, 3, \dots, J_{i+1} \end{aligned} \right. \tag{15} \] 将偏移项对应的输入值看看成1，即\(V_{O_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}} = 1\)，则上式中\(W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}\)可看成是常数1的权重，并且这个权重影响到的是\(O_{i,\ \ j{i}}\), 那么将此权重写作\(b_{i,\ \ j{i}}\)表示第\(i\)层\(j_{i}\)个神经元求和项的偏置 \[ \frac{\partial{\ell}}{\partial b_{i\ \ ,\ j_{i}}} = \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} \ \cdot \ f^\prime \big(V_{IN_{i\ ,\ j_{i}}} \ \big) \tag{16} \] 对于学习率为\(\alpha\)，batch为\(M\)的SGD权值逐层更新公式如下： \[ \boxed{ W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}} = W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}} - \alpha \cdot \sum_{m=1}^{M} \frac{\partial{\ell}}{\partial W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}} } \tag{17} \] 5.2.3. 反向传播简单例子 \[f(\boldsymbol{x}) = \frac{1}{1 + \exp^{-(w_{0} x_{0} + w_{1} x_{1} + w_{2})}}的BP示意图 \] 图2，BP示意图 5.3. Xavier参数初始化 2010 Xavier Glorot， Yoshua Bengio Understanding the difficulty of training deep feedforward neural networks, http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers/initializers http://blog.csdn.net/app_12062011/article/details/57956920 5.3.1. 问什么要使用初始化 主要目的是使的训练更容易进行下去 考虑计算机的数值运算时的上溢及下溢存在，对应避免方差过大导致剃度爆炸及剃度消失 5.3.2. 基本假设 输入样本x，权重w，偏置b分别服从均值为0的分布，所有样本属于独立同分布，所有权重属于独立同分布 关于原点对称的激活函数，即满足\(f^\prime(x) \approx 1\)，满足该条件的激励函数有Softsig， tanh， relu，所以以下讨论只是针对这些激励函数，而不包括sigmoid函数。 因为激励函数的0导数为1，且在x，w，b均值为0的假设下，每个神经元激励的输入的均值0附近。为了方便推导，将激励函数近似成线性函数 即，对于激励函数的输入\(In\),激励函数的出处\(Out\) \[Out=f(In)=In \tag{18}\] 5.3.3. 方差公式 用到的几个方差公式 如果有均值为0的两个互相独立的变量\(x\), \(y\)。则\(Var(xy) = Var(x) Var(y)\) 对于常数\(a\)，有\(Var(x+a) = Var(x)\) 5.3.4. 目标函数 5.3.4.1. 保证正向传播方差一致 任意两层网络激励函数的输出方差相等. 目标函数 \[ \forall(i, i^\prime)，\ \ Var[O^{i}] = Var[O^{i^\prime}] \tag{19} \] 根据式（2）， \[ IN_{i,\ j_i} = \sum_{j_{i-1}\ \ =1}^{J_{i-1}} W_{i-1\ \ , \ \ j_{i-1}\ \ ,\ \ j_{i}} \cdot O_{i-1\ \ , \ \ j_{i-1}} + b_{i\ ,\ j_{i}} \] 激励函数输入经过经过激励函数后得到激励函数的输出： \[ O_{i,\ j_i} = f(IN_{i,\ j_i}) \] 利用式（19）的激励函数的近似得 \[ O_{i,\ j_i} = \sum_{j_{i-1}\ \ =1}^{J_{i-1}} W_{i-1\ \ , \ \ j_{i-1}\ \ ,\ \ j_{i}} \cdot O_{i-1\ \ , \ \ j_{i-1}} + b_{i\ ,\ j_{i}} \] 计算方差 \[ Var(O_{i,\ j_i}) = \sum_{j_{i-1}\ \ =1}^{J_{i-1}} Var(W_{i-1\ \ , \ \ j_{i-1}\ \ ,\ \ j_{i}}) \cdot Var(O_{i-1\ \ , \ \ j_{i-1}} ) \] 合并相同方差 \[ Var(O_{i}) = J_{i-1} Var(W_{i-1}) \cdot Var(O_{i-1}) \] 可以将以上递推公式展开得到论文中的形式，但是使用递推公式既可得到结论 要使得目标函数成立，需使得每一层的神经元输出的方差都和下一层相同，即，需使得 \[ J_{i} Var(W_{i}) = 1 ， \ \ i = 1, 2, \cdots, n \tag{20} \] 5.3.4.2. 保证反向传播的方差一致 任意两层网络的损失损失对激励层输入的导数相等 目标函数： \[ \forall(i, i^\prime)，\ \ \frac{\partial{\ell}}{\partial IN_{i}} = \frac{\partial{\ell}}{\partial IN_{i^\prime}} \tag{21} \] 在式（5），式（6），式（7）的基础上可以容易得到： \[ \frac{\partial{\ell }}{\partial IN_{i\ ,\ j_{i}}} = \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ \cdot \ W_{i\ ,\ j_{i} \ \ , \ j_{i+1}} \ f^\prime \big(V_{IN_{i\ ,\ j_{i}}} \big) \ \cdot \ \frac{\partial{\ell}}{\partial IN_{i+1\ ,\ j_{i+1}}} \] 计算方差 \[ Var(\frac{\partial{\ell }}{\partial IN_{i\ ,\ j_{i}}}) = \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ \cdot \ Var(W_{i\ ,\ j_{i} \ \ , \ j_{i+1}}) \ \cdot \ Var(f^\prime \big(V_{IN_{i\ ,\ j_{i}}} \big)) \ \cdot \ Var(\frac{\partial{\ell}}{\partial IN_{i+1\ ,\ j_{i+1}}}) \] 相等方差合并 \[ Var(\frac{\partial{\ell }}{\partial IN_{i}}) = J_{i+1}\ \cdot \ Var(W_{i}) \ \cdot \ Var(\frac{\partial{\ell}}{\partial IN_{i+1}}) \] 则，要使得式（20）成立，需使得 \[ J_{i+1}Var(W_{i}) = 1， \ \ i = 1, 2, \cdots, n \tag{22} \] 5.3.5. 结论 当\(J_{i} \neq J_{i+1}\) 时式（20）和式（22）同时满足的话是矛盾的，但是论文中提出了一种折中方法，使用两种情况的调和平均数 \[ Var(W_{i}) = \frac{2}{J_{i} + J_{i+1}} \] 均值为0的均匀分布可以表示为[-a, a]。令其方差为\(\frac{2}{J_{i} + J_{i+1}}\)，则有 \[ \frac{(a-(-a))^2}{12} = \frac{2}{J_{i} + J_{i+1}} \] 得 \[ a = \frac{\sqrt{6}}{\sqrt{J_{i} + J_{i+1}}} \] 则 \[ w \sim U\Bigg[-\frac{\sqrt{6}}{\sqrt{J_{i} + J_{i+1}}}, \frac{\sqrt{6}}{\sqrt{J_{i} + J_{i+1}}}\Bigg] \tag{23} \] 5.3.6. 特别说明 式（23）是针对激励函数\(f\)，有\(f^\prime(x) \approx 1\)，且有\(f(0)=0\)条件的推导。基于以上两个条件近似f(x)=x。所以上面推导并不适用于sigmoid激励函数。 因为推导过程中激励函数使用了近似，另外正反向传播方差方差的不一致使用了折中的调和平均，所以针对特别深的网络Xavier初始化并不总是有用。 因为使用了激励函数的近似在0处的近似替代，则数据输入层尽可能在均值为0，可以去均值或归一化等。 5.4. Batch Normalization 主要参考：https://arxiv.org/abs/1502.03167 为了避免梯度消失导致深度神经网络无法训练的情况，通常使用relu激励函数+vavier的权值初始化方式 如果使用sigmoid的情况下，尽量使得各个神经元求和之后尽量为均值为0的高斯分布中， 将过激励函数的输入数据的每一个维的数据标准化到均值为0，方差为1的正太分布。即，使得\(\hat{x}^{(k)} \scriptsize{\sim} N(0, 1)\) 5.4.1. BN的作用 Batch Normalizaiton作用基本同Xavier初始化，不同的是，BN层不是从初始化考虑的，而是更直接在激励层输入之前强制标准化成固定均值及方差的高斯分布中。 整体来说有以下特点： 1，对初始化权重参数没有要求 2，允许训练过程以高学习率来训练网络而不至于提督消失或剃度爆炸 3，可以提高网络的泛化能力，降低对dropout的依赖 5.4.2. BN基本思想 1，将每一个维度的数据分布标准化到均值为0，方差为1的高斯分布中去 2，针对强行把数据分布拉到高斯分布的数据再进行线形变换，补偿一部分标准化过程中数据压缩。大概思维同卷积神经网络的卷积和池化层之后接全链接层 5.4.3. BN层的训练过程中的正向传播 使用m个样本更新参数时，m个样本的的某一个维度为例，下式中\(x_i\)代表第\(i\)个样本的第\(k\)个分量。\(\beta\)，\(\gamma\)为未知参数，每层每个分量的共享，需要通过训练。\(\mu\)，\(\sigma^2\):是由参与更新的的这个batch决定的 BN训练过程中的正向传播 5.4.4. BN层的使用 使用过程同正向传播，只是针对输入样本是其均值和方差由多个mini-batch的均值和方差的期望组成，既有N次mini-batch时： \[ \mu = E_B[\mu_{B}] \tag{24} \] \[ \sigma ^ 2 = \frac{m}{m-1}E_B[\sigma_{B} ^ 2] \tag{25} \] \[ \hat{x_i} = \frac{x_i - \mu}{\sqrt{\sigma ^ 2 + \epsilon}} \tag{26} \] \[ y_i = \gamma \hat{x_i} + \beta \tag{27} \] 其中\(\epsilon\)为较小的数，防止分母为0的情况发生 5.4.5. 含有BN层的BP传播 BN层的导数 则根据式（13） \[ \begin{aligned} &amp; \frac{\partial{\ell}}{\partial y_{i\ ,\ j_{i}}} = \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ f^\prime \big(V_{IN_{i+1\ ,\ j_{i+1}}} \ \big) \ \cdot \ W_{i\ ,\ j_{i} \ \ , \ j_{i+1}} \ \cdot \ \frac{\partial{\ell}}{\partial O_{i+1\ ,\ j_{i+1}}} \end{aligned} \tag{28} \] 则根据BN层的导数计算公式容易得到： \[ \frac{\partial{\ell}}{\partial x_{i\ ,\ j_{i}}} \tag{29} \] \[ \frac{\partial{\ell}}{\partial \gamma_{i\ ,\ j_{i}}} \tag{30} \] \[ \frac{\partial{\ell}}{\partial \beta_{i\ ,\ j_{i}}} \tag{31} \] 为了和式（13）保持一致，更新式（13）为 \[ \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} = \frac{\partial{\ell}}{\partial x_{i\ ,\ j_{i}}} \tag{32} \] 上式表示，损失对经过激励函数输出的剃度，至此含有BN层和没有BN层的反向传播从形式上得到统一 5.5. Dropout http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf 控制神经网络过拟合的一种方法 图2 dropout示意图 5.5.1. 基本思想 有性繁殖和无性繁殖的区别： 无性生殖因为其遗传的单一性，总能容易的把大段优秀的基因遗传给子代，而有性生殖却不太具备把大段优秀的基因传给下一代。但是有性繁殖是最先进的生物进化方式 论文中提出有性繁殖优于无性繁殖论可能原因， 长期自然进化选择协作能力强的基因的而不是个体，协作方式更加稳健。 另一种理解就是有性繁殖避免了整个基因库走向极端，而不同基因的组合方式将基因多使得样性更容易快速适应不确定的未来。 基于有性繁殖的被保留下来的特性，训练神经网络过程中，不是用所有的神经元，而是一部分神经元。所有有了Dropout 即：每个mini—bath训练时都以一定概率\(p\)随机打开若干神经元（不参与本次运算），多次训练即多种网络结构集成达到控制过拟合的目的 当\(p=0.5\)时熵达到最大，即网络结构数量的期望可以达到最大值，这应该也是论文中提到\(0.5\)的原因 5.5.1.1. 多模型集成 每次训练时神经网络结构都不同， 最后结果是通过多种结构叠加而成， 从这个角度来说Dropout有Bagging的思想 5.5.1.2. 权值共享 考虑针对多个网络结构单独训练参数带来的时间成本，Dropout使用了权值共享策略，即所有网络的权值是相应位置是同一个权值，只是当某些神经元关闭时，该神经元和下一层链接神经元之间的权值不参与更新。从多个模型参数有协同参与完成模型预测的角度来说， Dropout具有Boosting的思想 5.5.2. 算法基本流程 图2 正常网络和引入dropout网络示意图 如上图，引入Dropout的神经网络在训练阶段每个神经元之后多了一个开关阀， 即在一次参数更新时，不是所有的神经元被激活 5.5.2.1. 参数训练 初始化所有链接的权重 针对没次训练： 每个神经元以概率\(p\)的情况关闭，即以概率\(p\)打开神经网络的连接， 构建网网络，以之前权重为初始值更新本次有效链接的权值 5.5.2.2. 模型预测 所有神经元的权值乘以概率\(p\)，整个网络以未加入dorpout之前的结构进行正向传播进行结果预测。乘以概率\(p\)可以理解为，网络在训练过程中，每个神经元始终是以概率\(p\)参与最后的预测， 所以预测时需要以概率\(p\)来打开，从期望的角度来可以看作是以每个神经元都被打开但其最终作用降为原来的\(p\)倍，既保证了目标一致性的前提下，又使得神经元均参与数据预测，达到提高网络泛化能力的目的 假设训练结果得到某链接的权值为\(w\)，预测过程中该神经元的贡献为\(w \cdot x\)， 使用过程中该链接对应的值为\(p \cdot w \cdot x\)， 假设\(w = \frac{1}{p} \cdot w^\prime\)。则训练过程可以为\(\frac{1}{p} \cdot w^\prime\)，模型使用过程中\(w^\prime \cdot x\)。这样可以在使用过程中保证在含有Dropout网络输出的一致性，而不用单独处理 [深度学习中 Batch Normalizat为什么效果好]https://www.zhihu.com/question/38102762 5.6. 其他提高泛化的方法 L0-norm L1-norm L2-norm max-norm 提前终止 adsf 6. 参考 [1] 2016.11.11 Ian Goodfellow, Yoshua Bengio, Aaron Courville 《Deep Learning》 [2] 2017.03.15《Deep Learning翻译》https://exacity.github.io/deeplearningbook-chinese/ [3] 2015.03.02 Google Inc Batch Normalization [4] 2014, Hintorn,etcDropout: A Simple Way to Prevent Neural Networks from Overfitting [5] https://en.wikipedia.org/wiki/Backpropagation [6] CS231n Convolutional Neural Networks for Visual Recognition [7] 2010 Xavier Glorot， Yoshua Bengio Understanding the difficulty of training deep feedforward neural networks, http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf [8] TensorFlow.contrib.layers.xavier_initializer [9] 权重初始化Xavier [10] Must Know Tips/Tricks in Deep Neural Networks]]></content>
      <categories>
        <category>机器学习</category>
        <category>DL</category>
      </categories>
      <tags>
        <tag>DNN</tag>
        <tag>BP</tag>
        <tag>xavier</tag>
        <tag>BN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成方法]]></title>
    <url>%2F2017%2F04%2F04%2Fensemble-learning%2F</url>
    <content type="text"><![CDATA[Ensemble methods 集成方法是将多个基模型集成起来以提高最终模型的预测准确率或泛化能力的方法 1. 两种基本集成学习思路 1.1. Bagging Bootstrap aggregating 1.1.1. Bagging思想 给定一个大小为\(m\)的样本集\(D\)，Bagging算法从中均匀、有放回地选出\(n\)个大小为\(s\)的子集\(D_{i}\)，作为新的训练集。在这\(t\)个训练集上使用分类、回归等算法，则可得到 \(t\)个模型，再通过取平均值、投票等方法，即可得到Bagging的结果 1.1.2. 样本被抽到的概率 对于样本总量为m，有放回的的随机抽样\(t(t =n \cdot s)\)次， 任一样本至少抽到一次的概率： \[ P = 1 - (1 - \frac{1}{m})^{t} \] 特别的，当\(m \rightarrow +\infty\)且\(t＝m\)时，有如下： \[ \begin{aligned} P = &amp; 1 - (1 - \frac{1}{m})^m \\ = &amp; 1 - \frac{1}{(1 + \frac{1}{-m})^{-m}} \\ ＝ &amp; 1 - \frac{1}{e} \\ \approx &amp; 63.2％ \end{aligned} \] 上式中\(e\)为自然常数 。 1.1.3. 减小方差的说明 Bagging可以减小预测结果方差 简单说明，考虑以下极端情况 假设n个模型是完全独立的，即各模型任意组合的协方差为0 假设各个模型的方差均为\(Var(x)\)， 则 \[ \begin{aligned} &amp; Var\big[\frac{1}{n}(X_1 + X_2 + \ldots + X_n)\big] \\ = &amp; \sum_{i=1}^n var(\frac{1}{n}X_1) \\ = &amp; n \cdot \frac{1}{n^2} Var(x) \\ = &amp; \frac{1}{n} Var(x) \\ \end{aligned} \] 假设各模型完全相同，则： \[ Var\big[\frac{1}{n}(X_1 + X_2 + \ldots + X_n)\big] = Var(\frac{1}{n} \cdot n X_i) = Var(x) \] 现实情况介于两者之间，所以Bagging可以降低方差 1.2. Boosting 通过多个弱分类器集成为强分类器方法的统称 根据以上定义，Bagging是boosting的一个子集，但是很多资料上Boosting特指，各个模型是训练基于整体模型的误差训练的。如AdaBost是串行训练模型，第n＋1 个模型是针对前n个模型集成后的误差来训练的。下文也是将Boosting方法看成是多个模型基于误差协作组成强模型的的方法 Boosting的主要特点：弱分类器之间有依赖关系 因为Boosting的误差相关联性，所以Boosting是偏向于降低误差 2. Bagging算法 随机森林: Random Forests，https://zh.wikipedia.org/wiki/随机森林 ， 是Bagging的一种实现 训练n不剪枝，参与构建树的特征为抽样特征，对多对棵树进行投票或取平均 分类问题：多个ID3、C4.5、C5.0或CART分类树结果投票 回归问题：多个CART回归树结果求平均 3. Boosting算法 3.1. 向前分布算法 http://reset.pub/2017/03/31/forward-stagewise-algorith/ 3.2. AdaBoost http://reset.pub/2017/03/30/adaboost/ 3.3. 提升，梯度提升，梯度提升树 http://reset.pub/2017/04/03/gbdt/ 3.4. xgboost http://reset.pub/2017/04/01/xgboost/ 3.5. LightGBM 微软开源的梯度提升库 Light Gradient Boosting Machine http://www.msra.cn/zh-cn/news/blogs/2017/01/lightgbm-20170105.aspx http://www.msra.cn/zh-cn/news/blogs/2017/01/lightgbm-20170105.aspx 3.6. gcForest 周志华博士和其学生提出的深度森林模型。 http://reset.pub/2017/03/31/gcForest/ Boosting模型在构建构成中可以对每基模型使用样本、特征抽样。所以以上算法不一定是纯粹的Boosting 4. Stacking（融合模型） 常见stacking方法如：GBDT+LR、随机森林+LR 5. 参考 [1]《统计学习方法》，李航著，2012 [2]《机器学习》，周志华著，2016 [3] https://en.wikipedia.org/wiki/Gradient_boosting [4] XGboost的GitHub地址：https://github.com/dmlc/xgboost [5] Deep Forest: Towards An Alternative to Deep Neural Networks，Zhi-Hua Zhou and Ji Feng，2017.02.28 [6] XGBoost官网：https://xgboost.readthedocs.io/en/latest/model.html [7] XGboost的GitHub地址：https://github.com/dmlc/xgboost [8] http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf [9] https://www.zhihu.com/question/26760839 [10] https://zh.wikipedia.org/wiki/E_(数学常数) [11] https://github.com/Microsoft/LightGBM [12] http://www.msra.cn/zh-cn/news/blogs/2017/01/lightgbm-20170105.aspx]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成方法</category>
      </categories>
      <tags>
        <tag>集成方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gradient Boosting]]></title>
    <url>%2F2017%2F04%2F03%2Fgbdt%2F</url>
    <content type="text"><![CDATA[Gradient Boosting，梯度提升 提升，梯度提升，梯度提升树 1. Boosting，提升 梯度提升树是有多棵树共同决策而成的 # 模型 \[F_m(x) = \sum_{i=1}^m f_m(x) = F_{m-1}(x) + f_m(x) \tag{1}\] \(F_m(x)\)：由m个子模型构成的模型 \(f_m(x)\)：第m个子模型 构建过程： 1，初始化 \[F_0(x) = \mathop{\arg\min}\limits_{\gamma} \sum_{i=1}^n L(y_i, \gamma) \tag{2}\] \(L(y_i, \gamma)\)：损失函数 \(y_i\)：回归目标 2，训练第m棵树\[ F_m(x) = F_{m-1}(x) + \mathop{\arg\min}\limits_{f} \sum_{i=1}^n L\big(y_i, F_{m-1}(x_i) + f(x_i)\big) \tag{3} \] 1.1. 平方损失的提升 对于损失函数\(L(y_i, \hat{y}_i) = (y_i - \hat{y}_i) ^2时\) 第m棵树构建时，第i个样本取得最小值时，须使得损失函数一阶导在\(F_{m-1}(x_i) + f_m(x_i)\)处等于零，即： \[2 \cdot \big(y_i - F_{m-1}(x_i) - f_m(x_i)\big)= 0\] 即，\[f_m(x_i) = y_i - F_{m-1}(x_i)\] 算法步骤 初始化\(f_0(x) = \mathop{\arg\min}\limits_{\gamma} \sum_{i=1}^n (y_i, \gamma)^2 = \frac{1}{n} x\) for m in 1，2，\(ldots\), M: # 建立第m棵树 \(r_{mi} = y_i - f_{m-1}(x_i), i = 1,2, \cdots, N\) 拟合残差向量\(r_{m}\)学习一个回归树，得到\(f_m(x)\) \(F_m(x) = F_{m-1}(x) + f_m(x)\) \(F_M(x) = \sum_{m=1}^M f_m(x)\) 2. Gradient boosting，梯度提升 2.1. 问题表达 梯度提升是提升在损失函数不方便直接求极值时的扩展。梯度提升使用梯度下降求极值 考虑 \(f = \min l(\theta)\)，\(l(\theta)\)是凸的。 利用梯度下降，步长为\(\gamma\)的梯度下降为： \[ \theta^{(t)}= \theta^{(t-1)} - \gamma \bigg[\frac{ \partial l(\theta)} {\partial \theta}\bigg]_{\theta = \theta^{(t-1)} } \] 原问题可以重新表达为在已知\(F_{m-1}(x)\)的情况下，更新参数\(F_{m}(x)\)替代\(F_{m-1}(x)\)，使得\(L \big(y_i, F(x)\big)\)最小化，则，可以利用梯度下降的方式： \[ F_m(x) = F_{m-1}(x) - \bigg( \gamma_m \sum_{i=1}^n \frac{\partial L(y_i,Z_i)}{\partial Z_i)} \bigg)_{Z_i = F_{m-1} \ \ \ (x_i)} \tag{4} \] \[ \gamma_m = \mathop{\arg\min}\limits_{\gamma} \sum_{i=1}^n \Bigg( F_{m-1}(x_i) - \gamma \bigg[ \frac{\partial L(y_i,Z_i)}{\partial Z_i} \bigg]_{Z_i = F_{m-1} \ \ (x_i) } \Bigg) \tag{5} \] \(\gamma_m\) 可以通过线性搜索得到 2.2. 伪代码 图1 梯度提升伪代码 3. Gradient Boosting Decision Tree， 梯度提升树 梯度提升树是梯度提升的一个特例，基模型由决策回归树构建 给定一棵树，任何样本样本在树的规则下映射到叶子结点，叶子结点的值即为该样本的预测值，假设有\(J_m\)个叶子结点，每个叶子的值为\(b_{jm}\)，则树的输出\(h_m(x)\)可以写成 \[ h_m(x) = \sum_{j=1}^{J_m} b_{jm} I (x \in R_{jm}) \tag{6} \] 如果将\(h_m(x)\)看成梯度提升中拟合的伪残差\(r_{mi}\)， 则梯度提升树可以表示为： \[ F_m(x) = F_{m-1}(x) + \gamma_m h_m(x) \] 将\(h_m(x)\)带入上式得： \[ \gamma_m = \mathop{\arg\min}\limits_{\gamma} \sum_{i = 1}^n L(y_i, F_{m-1(x_i)} + \gamma h_m(x_i)) \] \[ F_m(x) = F_{m-1}(x) + \gamma_m \sum_{j=1}^{J_m} b_{jm} I (x \in R_{jm}) \] 令：\[\gamma_{jm} = \gamma_m \cdot b_{jm}\] 则： \[ F_m(x) = F_{m-1}(x) + \sum_{j=1}^{J_m} \gamma_{jm} I (x \in R_{jm}) \tag{7} \] \[ \gamma_{jm} = \mathop{\arg\min}\limits_{\gamma} \sum_{x_i \in R_{jm}} L(y_i, F_{m-1}(x_i) + \gamma) \tag{8} \] 4. 模型改进 1，修改模型权重 \[F_m(x) = F_{m-1} + \nu \cdot \gamma_m h_m(x)， 0 &lt; \nu \leqslant 1 \tag{9}\] 引入后需要更多的子模型个数 2，使用随机梯度下降代替梯度下降 3，加入bagging采样构建每个子模型 4，增加了模型复杂度 5. 参考资料 [1]《统计学习方法》，李航著，2012 [2] https://en.wikipedia.org/wiki/Gradient_boosting [3] XGboost的GitHub地址：https://github.com/dmlc/xgboost]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成方法</category>
      </categories>
      <tags>
        <tag>集成方法</tag>
        <tag>Boosting</tag>
        <tag>GB</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2017%2F04%2F01%2Fxgboost%2F</url>
    <content type="text"><![CDATA[eXtreme Gradient Boosting XGBoost是GBDT的一种实现的代码库，同样也是集成方法的一种 相对原始GBDT主要有以下改进 在目标函数中加入模型复杂度 利用泰勒展开式的前两项作为目标函数的近似 损失函数自定义 XGboost快于GBDT的说明： 1，XGboost使用二阶泰勒展开近似目标，使用函数的极值处导数为零，可以一步得到全局极值的近似。 2，GBDT是给定一个当前收敛最快的方向，每次走一步调整一步，需要多个完成。 基于以上两点XGBoost的收敛速度快于GBDT。 这也是也是牛顿法收敛速度快于梯度下降的原因 XGBoost 是由多个回归树boosting而成的结果 1. 算法说明 1.1. XGBoost中决策树的目标函数 决策树算法的目标：损失+正则 \[Obj(\Theta) = \min \bigg[\ L(\theta) + \Omega(\Theta) \bigg] \tag{1}\] XGBoost本身是有多棵回归树构成，回归树的不同损失函数决定了XGBoost是用于分类还是回归 损失函数： 以下描述中\(\hat{y}_i\)为预测结果 对于回归问题： \[L(\theta) = \sum_i (y_i-\hat{y}_i)^2 \tag{2}\] 对于二分类问题 损失函数 \[L(\theta) = \sum_i \big[ y_i\ln (1+e^{-\hat{y}_i}) + (1-y_i)\ln (1+e^{\hat{y}_i})\big] \tag{3}\] \(y_i = 0, 1。\hat{y}_i \in (-\infty，+\infty)\) 预测分类 \[sigmoid \bigg( \frac{1}{1+e^{-\hat{y}_i}} \bigg)\] 多分类 损失函数 \[ L(\theta) = -\frac{1}{m}\bigg[ \sum_{i=1}^m \sum_{j=1}^k 1 \{y_i= j\} \cdot \log \frac{e^{\hat{y}_{i\_j}} }{\sum_{l=1}^k e^{\hat{y}_{i\_l}} } \bigg] \tag{4} \] \(m\)：样本个数 \(k\)：分类个数 \(1 \{y_i= j\}\)：\(1 \{值为真的表达式\} = 1\) \(\hat{y}_{i\_j}\)：样本\(x_i\)在样本第j类上的预测值 预测分类 \[arg \max_{j} \frac{e^\hat{y}_{i\_j}}{\sum_{l=1}^k e^{\hat{y}_{i\_l}}} \ \ (j = 1, 2, \ldots, k)\] 正则项 \[\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2 \tag{5}\] \(T\)：叶子节点个数 \(w_j\)：叶子节点代表的回归值，例：CART回归树中落在某一叶子节点\(y_i\)的平均值 \(\gamma\)：超参数，叶子节点个数的惩罚系数 \(\lambda\)：超参数，L2-norm平方的系数 考虑boosting算法的一般形式： \[\begin{split}\hat{y}_i^{(0)} &amp;= 0\\ \hat{y}_i^{(1)} &amp;= f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\ \hat{y}_i^{(2)} &amp;= f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\ &amp;\dots\\ \hat{y}_i^{(t)} &amp;= \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i) \end{split} \tag{6}\] 则 \[ \begin{split}\text{obj}^{(t)} &amp; = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i) \\ &amp; = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + constant \end{split} \tag{7} \] 1.2. 决策树决策规则 1.2.1. 目标函数的二阶近似 泰勒展式： \[ f(x) = f(a) + \frac{f&#39;(a)}{1!}(x-a)+ \frac{f^{2}(a)}{2!}(x-a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n + R_n(x) \] \(\begin{aligned} 令， &amp;f(x) = l\big(y_i， \hat{y}_i^{(t-1)} + f_t(x_i)\big)，f(a) = l(y_i， \hat{y}_i^{(t-1)}) \\ 则，&amp; x - a = f_t(x_i) \end{aligned}\) 则目标函数的二阶泰勒展开为： \[ \text{obj}^{(t)} \approx \sum_{i=1}^n \big[ l(y_i， \hat{y}_i^{(t-1)}) + g_i \cdot f_t(x_i) + \frac{1}{2} \cdot h_i \cdot f_t^2(x_i)\big] +\sum_{i=1}^t\Omega(f_i) \tag{8} \] \[ \begin{aligned} 令： &amp; g_i = \frac{\partial l(y_i， \hat{y}_i)}{\partial \hat{y}_i} \ \Bigg|_{\hat{y}_i = \hat{y}_i^{(t-1)}} \\ &amp; h_i = \frac{\partial ^2 l(y_i， \hat{y}_i)}{\partial \hat{y}_i} \ \Bigg|_{\hat{y}_i = \hat{y}_i^{(t-1)}} \end{aligned} \tag{9} \] 定义好损失函数，前\(t-1\)棵树训练好后，\(g_i\) 和 \(h_i\)就确定了。通过\(g_i\) 和 \(h_i\)是调整样本权值，用于训练第\(t\)棵树，也体现Boosting思想 考虑到是对\(\text{obj}^{(t)}\)求最小值，前\(t-1\)棵确定下来后\(l(y_i， \hat{y}_i^{(t-1)})\)为定值，另外前\(t-1\)棵树的正则项也为常数，即对于目标而言 \(\sum_{i=1}^t\Omega(f_i) = constant + \Omega(f_t)\) 所以目标可以表示为： \[ \begin{split} \text{obj}^{(t)} &amp; \approx \sum_{i=1}^n l(y_i， \hat{y}_i^{(t-1)}) + \sum_{i=1}^n \big[g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)\big] + \Omega(f_t) \\ &amp; = \sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) ] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2\\ \end{split} \tag{10} \] 其中\(f_t(x_i)\)表示样本\(x_i\)在第\(t\)棵树上的预测结果，假设设n个样本在第\(t\)棵树上的预测结果分布在\(T\)个叶子节点上，则某一叶子节点\(I_j\)上有必相同的回归值\(w_j\)，则有 \[ \begin{split} \sum_{i\in I_j} g_i f_t(x_i) = (\sum_{i\in I_j} g_i) \cdot w_j \\ \end{split} \tag{11} \] 则： \[ \begin{split} \text{obj}^{(t)} \approx \sum_{i=1}^n l(y_i， \hat{y}_i^{(t-1)}) + \sum^T_{j=1} [(\sum_{i\in I_j} g_i) w_j + \frac{1}{2} (\sum_{i\in I_j} h_i + \lambda) w_j^2 ] + \gamma T \end{split} \tag{12} \] \[ \begin{aligned} 令：&amp;G_j = \sum_{i\in I_j} g_i\\ &amp;H_j = \sum_{i\in I_j} h_i \end{aligned} \tag{13} \] \[ \text{obj}^{(t)} \approx \sum_{i=1}^n l(y_i， \hat{y}_i^{(t-1)}) + \sum^T_{j=1} \big[G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2\big] +\gamma T \tag{14} \] 1.2.2. 目标函数取得极值时的条件 当\(\text{obj}^{(t)}\)取得极小值时， \(\frac{\partial \text{obj}^{(t)}}{\partial w_j} = 0\)，则： \[ w_j^\ast = -\frac{G_j}{H_j+\lambda} \tag{15} \] \(w_j^\ast\)即为第t棵树，落在第j个叶子结点预测值\(f_t(x_i)\) 则决策树的损失为： \[ \text{obj}^\ast = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T \tag{16} \] 1.2.3. 决策树建立过程的参考依据 决策树损失越小越好，类似基尼系数或熵。则决策树某个节点分裂前后的的增益为：父节点点的损失 - 左子树的损失 - 右子树的损失。不分裂：全部样本子一个叶子结点；分裂：左子树（叶）和右子树（叶）都先看成叶子，则\(T=1\)，决策树分裂前后的增益为： \[ Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma \tag{17} \] 决策树建立过程是寻找使得增益\(Gain\)最大的特征及特征上的值过程 2. XGBoost基本思想伪代码 1234567891011121314定义损失函数L初始化预测结果为f_0求所有样本的一阶、二阶损失for t in 1，2，..., n： # 训练第i棵树 1，根据式（9）更新样样本的一阶、二阶损失 2，根据式（17）建立决策树并存储树的结构及叶子节点的值 3，按叶子根据式（15）计算每个样本在第t棵树上的预测值 f_t(x_i) 4，根据式（6）y_t = y_(t-1) + f_t(x_i)输出模型 具体XGBoost实现参数见：https://github.com/dmlc/xgboost/blob/master/doc/parameter.md 3. 参考资料 [1] XGBoost官网：https://xgboost.readthedocs.io/en/latest/model.html [2] XGboost的GitHub地址：https://github.com/dmlc/xgboost [3] http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf [4] https://en.wikipedia.org/wiki/Gradient_boosting [5] https://zh.wikipedia.org/wiki/泰勒公式 [6] http://ufldl.stanford.edu/wiki/index.php/Softmax回归]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成方法</category>
      </categories>
      <tags>
        <tag>集成方法</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gcForest]]></title>
    <url>%2F2017%2F03%2F31%2FgcForest%2F</url>
    <content type="text"><![CDATA[multi-Grained Cascade Forest 深度级联森林：决策树模型具有可解释性强的优点，多个决策树构成随机森林，多层多个森林构成深度森林 gcForest是西瓜书作者周志华博士和冯霁博士提出的一基于随机森林的深度森林的方法，尝试用深度森林的方法解决深度神经网络中存在的问题： 需要大量样本才能使得深度神经网络有较好的性能 调参困难 对硬件性能要求较高 论文中实验了在小数据集上gcForest，取得了不错的效果 论文地址：https://arxiv.org/pdf/1702.08835.pdf 部分翻译：http://it.sohu.com/20170302/n482153688.shtml 1. 算法基本思路 gcForest是西瓜书作者周志华博士和冯霁博士提出的一基于随机森林的深度森林的方法，是一种“ensemble of ensembles”的方法。类似深度神经网络，深度森林是每层是由多个随机森林组成，每层的随机森林是由完全随机森林及随机森林组成 完全随机森林的构建：构建1000个(超参数)完全随机树。完全决策树的构建过程：对于所有的特征，随机选择特征，随机选择特征下的split值，一直生长，直到每个叶节点包含相同的类别或者不超过10（超参数）个时，决策树训练停止 随机森林的构建：构建1000个(超参数）决策树。决策树的构建过程：从\(d\)个特征，随机抽取\(\sqrt{d}\)个特证，由gini系数做为特征选择及分裂的标准构建CART决策树 每一层经过多个森林处理的输出作为下级的输入，当到达某一层没有明显的性能提升(超参数)时，级连森林停止生长 2. 算法流程 2.1. 类向量的训练 （图1，类向量生成示意图) 图片说明：图中有两个小bug，第一个决策树“红圈”内的叶子节点少了一个“椭圆”，第三个决策树“红圈”内应该全为“正方形” 假设为k分类问题 1，针对已经训练好的森林中的树时，记录每个叶子节点的样本类别，按类别统计叶子节点的权重得到k维向量，树模型的每个叶子节点都对应一个k维向量(带key的向量，如，label_1：0.5, label_2:0.3, lable_3=0.2)。 2，给定一个样本经过树的运算到达叶子节点，对应一个k维向量，一个随机森林中对应1000个k维向量， 将1000个k维向量按照类别求平均，平均后的k维向量即为该样本在该森林上的类向量 2.2. 级连森林训练 （图2，级连森林模型示意图。 蓝色：随机森林。黑色：完全随森林) 伪代码如下： 123456789101112131415161718192021222324252627def model(input_data_list_list, label_list): """ input_data_list_list: 训练样本列表 label_list: 标签列表 """ i ＝ -1 ＃ 第i层级连森林 feture_list_list = [] model_list_list = [] feture_list_list[0] = input_data performance = 0 ＃ 初始化准确率 while 1: i += 1 modle_complete_rf_list, modle_rf_list &lt;- 以feture_list[i]为特征， label_list为样本标签，并行训练完全随机森林和随机森林 model_list_list[i] = [modle_complete_rf_list, modle_rf_list] # 统计该层的性能（如：正确率，准确率，召回率等） new_performance &lt;- 根据各森林的每个树的叶子节点统计性能 # 对比上层森林群的结果，对比上层性能增加是否达到阈值theta（阈值： 超参数） if new_performance - performance &gt; theta: return model_list_list ＃ 输出模型 # 更新性能统计 performance = new_performance 2.3. 级连森林预测 伪代码如下： 12345678910111213def fit(x, model_list_list): ''' x：待预测样本 model_list_list：级连森林模型，model_list_list[0]：列表，代表第0层的森林模型列表 ''' feture_list = x # 循环每层级连森林 for model_list in model_list_list: class_vector_list &lt;- 获得特征向量在m+n个森林上的m＋n个类向量 feture_list &lt;- 串行化类向量class_vector_list result &lt;- 按标签分组，分别求权重均值。找出最大平均值对应的类别即为预测结果 3. 使用多粒度扫描做特征处理 多粒度扫描结构 （图3，多粒度扫描示结构意图) 带多粒度扫描的级连森林结构 （图4，带多粒度扫描的增强级连森林示结构意图) 描述： 类似卷机神经网络的Pooling，深度森林也引入“滑动窗口”，替代pooling层的方法max-pooling, mean-pooling的计算方式为多个森林（完全随机森林和随机森林）后的类向量串行，具体过程大概如下： 每次滑动窗口选出来的特征经过随机森林和完全随机森林经过多个森林建模后得到类向量，串联类向量作为新的特征作为下一级的输入层 假设原始特征长度为m，滑动窗口长度为n(n &lt; m)，滑动窗口个数：[n, m]即共有m-n+1个滑动窗口 可以并行接入不同窗口做Pooling操作 4. 参考资料 [1] Deep Forest: Towards An Alternative to Deep Neural Networks，Zhi-Hua Zhou and Ji Feng，2017.02.28 [2] http://it.sohu.com/20170302/n482153688.shtml]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成方法</category>
      </categories>
      <tags>
        <tag>集成方法</tag>
        <tag>gcForest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前向分步算法]]></title>
    <url>%2F2017%2F03%2F31%2Fforward-stagewise-algorith%2F</url>
    <content type="text"><![CDATA[Forward Stagewise Algorithm 1. 算法描述 输入： 训练数据集：\(T＝{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)}\) 损失函数：L(y, f(x))。\(y\)：样本标签向量，\(f(x)\)：预测结果向量 基函数集：\({b(x, \gamma)}\)。\(\gamma\)：模型参数向量， 一组\(\gamma\)对应一个子模型 输出： 训练M个模型\(b(x, \gamma_m)\)，按模型权重\(\beta_m\)相加得到最红加法模型，如下： \[ \boxed{ f(x) = \sum_{i=1}^M \beta_m b(x, \gamma_m) } \] 2. 算法流程: \(f_0(x) = 0\) for m in 1, 2, \(\ldots\), M: \[ \begin{multline} (\beta_m, \gamma) = arg \min_{\beta, \gamma} \sum_{i=1}^m(y_i, f_{m-1}+\beta b(x_i;\gamma) \end{multline} \] \[ \begin{multline} f_m(x) = f_{m-1}(x) + \beta_m b(x; \gamma_m) \end{multline} \] \(f(x) = f_m(x)\) 3. 参考资料 [1]《统计学习方法》，李航著，2012]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成方法</category>
      </categories>
      <tags>
        <tag>集成方法</tag>
        <tag>前向分步算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2017%2F03%2F30%2Fadaboost%2F</url>
    <content type="text"><![CDATA[Adaptive Boosting AdaBoost是多个分类器组合算法，维基百科AdaBoost算法过程 1. 算法描述 输入： 训练数据集：\(T＝{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)}\) 基函数模型 输出： 训练M个模型\(G_m(x), m \in [1, M]\)，按模型权重\(\alpha_m, m \in [1, N]\)相加得到最红加法模型，如下： \[ \sum_{i=1}^M \alpha_m G_m(x) \] 2. 算法流程 初始化训练数据权重 \(w_{1i}=\frac{1}{N}，i= 1,2,\ldots,N\) for m in 1, 2, \(\ldots\), M #(训练M个基分类器): 按样本权重分布\(w_{m}\)采样\(D_m\)训练集 在\(D_m\)上训练基本分类器\(G_m(x)\) 计算\(G_m(x)\)在训练数据集上的分类误差率： \[ e_m = P\big(G_m(x_i) \neq y_i\big) = \sum_{i=1}^N w_{mi}I \big(G_m(x_i) \neq y_i\big) \] 计算基本分类器 \(G_m(x)\)的权重： \[\alpha_m = \frac{1}{2}log \frac{1 - e_m}{e_m}\] 更新样本权重 \[ w_{m+1,i} = \frac{w_{m,i}}{Z_m} \times \left\{ \begin{aligned} &amp; e^{-\alpha_m}，&amp; if G_m(x_i) = y_i \\ &amp; e^{\alpha_m}，&amp; if G_m(x_i) \neq y_i \\ \end{aligned} \right. \] \[ Z_m = \sum_{i=1}^N w_{m,i} \times \left\{ \begin{aligned} &amp; e^{-\alpha_m}，&amp; if G_m(x_i) = y_i \\ &amp; e^{\alpha_m}，&amp; if G_m(x_i) \neq y_i \end{aligned} \right. \] 输出分类器 \[ \boxed{ G(x) = sign(f(x)) = sign \Big(\sum_{m=1}^M \alpha_m G_m(x)\Big) } \] 3. 样本权重调整方法 对于样本按样本权重抽样 优点：样本集可以直接用现成的分类模型去拟合 缺点：对于样本权重 0.3333这种需要抽样凑成整数时个样本量较大 ，或者样本权重有损失 修改分类器损失或特征选择时的条件(决策树) 优点：可以较好的保持样本权重 缺点：需要造基本分类器轮子 4. AdaBoost与向前分布算法的关系 AdaBoost是[向前分步算法]http://reset.pub/2017/03/31/forward-stagewise-algorith/ 算法的特例，是由基本分类器组成的加法模型，损失函数为指数函数 5. 参考资料 [1]《统计学习方法》，李航著，2012]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成方法</category>
      </categories>
      <tags>
        <tag>AdaBoost</tag>
        <tag>集成方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVR]]></title>
    <url>%2F2017%2F03%2F26%2Fsvr%2F</url>
    <content type="text"><![CDATA[SVR，Support Vector Regression，支持向量回归 1. SVR问题引出 （图1， 左图：支持向量回归示意图及epsilon回归带示意图；右图：epsilon不敏感损失 有样本集 \(D=\{(\boldsymbol{x_1}，y_1)，(\boldsymbol{x_2}，y_2)，\ldots，(\boldsymbol{x_m}，y_m)\}\)。其中， \(\boldsymbol{x_i}\)是一个样本，列向量；\(y_i\)为回归目标，\(y_i \in \mathbb{R}\)。 找到一个回归模型\(f(x) = \boldsymbol{w}^T\boldsymbol{x} + b\)，使得\(f(\boldsymbol{x})\)与\(y\)在损失为\(\epsilon\)不敏感损失下尽可能接近，其中回归线由落在\(\boldsymbol{w} \cdot \boldsymbol{x} + b - \epsilon = 0\)与\(\boldsymbol{w} \cdot \boldsymbol{x} + b + \epsilon = 0\)区域内的点参与构建 2. 数学表达 假设样本结果为y, 预测结果为\(\widehat{y}\)，定义\(\epsilon\)不敏感损失为： \[ L_\epsilon(y, \widehat{y}) = \left\{ \begin{aligned} &amp; 0 &amp; if \ \ &amp; \lvert y - \widehat{y} \rvert &lt; \epsilon \\ &amp; \lvert y - \widehat{y} \rvert - \epsilon &amp; &amp; otherwise \end{aligned} \right. \tag{1} \] 上式 \(\epsilon &gt; 0\)，epsilon的意义：在离回归线上下\(\epsilon\)都不计入损失，其回归线可以看成回归线上下\(\epsilon\)区域边界组成的回归带，落在在回归带上的点x损失为0，预测\(\widehat{y}\)为固定x后回归带的中点 误差为epsilon不敏感，使用L2正则的模型为： \[J = C \sum_{i=1}^m L_\epsilon(y, \widehat{y}) + \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 \tag{2}\] 2.1. \(2\epsilon\)回归带能够覆盖所有点的情况 对于固定的\(\epsilon\)，所有点均在回归带上的情况： \[ \left\{ \begin{aligned} obj\ :\ \ &amp; \min_{\boldsymbol{w},b,\epsilon} \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 \\ st\ :\ \ &amp; \lvert f(\boldsymbol{x_i})- y_i \rvert \leqslant \epsilon \end{aligned} \right. \tag{3} \] 2.2. \(2\epsilon\)回归带不能够覆盖所有点的情况 参照SVM的线性不可分的情况，为每个样本引入松弛因子\(\xi_i\)使得不在回归带的点满足约束，并且在目标函数中加入对松弛因子的惩罚项\(C\)，则有 \[ \left\{ \begin{aligned} obj\ :\ \ &amp; \min_{\boldsymbol{w},b,\epsilon} \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 ＋ C \xi_i \\ st\ :\ \ &amp; \lvert f(\boldsymbol{x_i}) - y_i \rvert \leqslant \epsilon + \xi_i \\ &amp; \xi_i \geqslant 0 \end{aligned} \right. \tag{4} \] 3. 问题求解 3.1. 将转化为凸优化问题 考虑上式约束不连续可导，分情况去掉绝对值： \[ \begin{eqnarray} 隔离带上方的样本：&amp; f(x_i) &amp; &amp; &amp; &lt; &amp; y_i &lt; f(x_i) + \epsilon + \xi^+_i \\ 隔离带下方的样本：&amp; f(x_i) &amp; - \epsilon &amp; - \xi^-_i &amp; &lt; &amp; y_i &lt; f(x_i) \\ 隔离带上的样本： &amp; f(x_i) &amp; - \epsilon &amp; &amp; \leqslant &amp; y_i \leqslant f(x_i) + \epsilon \end{eqnarray} \tag{5} \] 综上约束条件可以化为： \[ f(x_i) - \epsilon - \xi^-_i \leqslant y_i \leqslant f(x_i) + \epsilon + \xi^+_i \] \(\xi^-_i \geqslant 0， \xi^+_i \geqslant 0\)。如果要求上下界是紧的，松弛因子至多有一个不为0，即\(\xi^-_i \xi^+_i=0\) 式(4)中将\(f(x) = \boldsymbol{w} \cdot \boldsymbol{x} + b\)，替换等价约束可以转化为： \[ \left. \begin{aligned} obj\ :\ \ &amp; \min_{\boldsymbol{w},b,\boldsymbol{\xi}} \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 ＋ C\sum_{i=1}^m(\xi^+_i ＋ \xi^-_i ) \\ st\ :\ \ &amp; \boldsymbol{w} \cdot \boldsymbol{x_i} + b - \epsilon - \xi^-_i - y_i \leqslant 0\\ &amp; y_i - \boldsymbol{w} \cdot \boldsymbol{x_i} - b - \epsilon - \xi^+_i \leqslant 0 \\ &amp; -\xi^+_i \leqslant 0 \\ &amp; -\xi^-_i \leqslant 0 \\ &amp; C，\epsilon 为超参数，C &gt;0，\epsilon \geqslant 0 \end{aligned} \right. \tag{6} \] 对于一个样本点\(x_i\)其松弛因子\(\xi^+_i\)和\(\xi^-_i\)至多只有一个大于0 式(6)为不带等式约束的凸优化 3.2. 拉格朗日函数 引入拉格朗日乘子\(\alpha_i^＋ \geqslant 0\)， \(\alpha_i^－ \geqslant 0\)，\(\mu^+_i \geqslant 0\)和\(\mu^-_i \geqslant 0\)，则拉格朗日函数为： \[ \begin{aligned} L(\boldsymbol{w},b,\boldsymbol{\xi^+},\boldsymbol{\xi^-},\boldsymbol{\alpha^+}, \boldsymbol{\alpha^-},\boldsymbol{\mu^+},\boldsymbol{\mu^-})= &amp; \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 ＋ C\sum_{i=1}^m(\xi^+_i ＋ \xi^-_i ) \\ &amp; + \sum_{i=1}^m \alpha_i^-(\boldsymbol{w} \cdot \boldsymbol{x_i} + b - \epsilon - \xi^-_i - y_i)\\ &amp; + \sum_{i=1}^m \alpha_i^+(y_i - \boldsymbol{w} \cdot \boldsymbol{x_i} - b - \epsilon - \xi^+_i ) \\ &amp; - \sum_{i=1}^m \mu_i^+ \xi_i^+ \\ &amp; - \sum_{i=1}^m \mu_i^- \xi_i^- \end{aligned} \tag{7} \] 3.3. 拉格朗日对偶函数 3.3.1. 拉格朗日函数极小问题 则，\(L(\boldsymbol{w},b,\boldsymbol{\xi^+},\boldsymbol{\xi^-})\)取最极小值时满足（KKT条件之一），以下公式均为取得最优值满足的条件，例：\(\boldsymbol{w}\) 实际表示 \(\boldsymbol{w^*}\)，可参考SVM： \[ \left\{ \begin{eqnarray} &amp; \nabla_{\boldsymbol{w}}L = 0 &amp; \Longrightarrow &amp; \boldsymbol{w} + \sum_{i=1}^m a_i^- \boldsymbol{x_i} - \sum_{i=1}^m \alpha_i^+ \boldsymbol{x_i} = 0 \ \ \ &amp; (1) \\ &amp; \nabla_{\boldsymbol{b}}L = 0 &amp; \Longrightarrow &amp; \sum_{i=1}^m \alpha_i^+ - \sum_{i=1}^m a_i^- ＝ 0 &amp; (2)\\ &amp; \nabla_{\boldsymbol{\xi_i^+}}L = 0 &amp; \Longrightarrow &amp; C - \alpha_i^+ - \mu_i^+ = 0 &amp; (3) \\ &amp; \nabla_{\boldsymbol{\xi_i^-}}L = 0 &amp; \Longrightarrow &amp; C - \alpha_i^- - \mu_i^- = 0 &amp; (4)\\ \end{eqnarray} \tag{8} \right. \] 整理上式得： \[ \begin{aligned} L = &amp; \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 +\boldsymbol{w} \sum_{i=1}^m(a_i^- \boldsymbol{x_i} + a_i^+ \boldsymbol{x_i}) \\ &amp; + (C - a_i^+ - u_i^+)\sum_{i=1}^m \xi_i^+ \\ &amp; + (C - a_i^- - u_i^-)\sum_{i=1}^m \xi_i^- \\ &amp; + b\sum_{i=1}^m(a_i^- - a_i^+) \\ &amp; - \epsilon\sum_{i=1}^m (a_i^- + a_i^+) \\ &amp; - \sum_{i=1}^m y_i(a_i^- - a_i^+) \\ \end{aligned} \tag{9} \] 将式(8)带入拉格朗日函数得拉格朗日对偶函数为： \[ \begin{aligned} \max_{\boldsymbol{a^+},\boldsymbol{a^-}}L = &amp; -\frac{1}{2}\lVert\boldsymbol{w}\rVert^2 - \epsilon\sum_{i=1}^m (a_i^- + a_i^+) - y_i \sum_{i=1}^m(a_i^- - a_i^+) \\ = &amp; -\frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m(a_i^- - a_i^+)(a_j^- - a_j^+)\boldsymbol{x}_i \cdot \boldsymbol{x}_j \\ &amp; - \epsilon\sum_{i=1}^m (a_i^- + a_i^+) - y_i \sum_{i=1}^m(a_i^- - a_i^+) \end{aligned} \tag{10} \] 3.3.2. 拉格朗日极大极小问题 考虑除(8)以外的KKT条件： \[ \left\{\ \ \ \begin{aligned} &amp; \alpha_i^-(\boldsymbol{w} \cdot \boldsymbol{x_i} + b - \epsilon - \xi^-_i - y_i) = 0 &amp; \ \ &amp; (1) \\ &amp; \alpha_i^+(y_i - \boldsymbol{w} \cdot \boldsymbol{x_i} - b - \epsilon - \xi^+_i ) = 0 &amp; &amp; (2) \\ &amp; \mu_i^+ \xi_i^+ = 0 &amp; &amp;(3)\\ &amp; \mu_i^- \xi_i^- = 0 &amp; &amp;(4) \\ &amp; \boldsymbol{w} \cdot \boldsymbol{x_i} + b - \epsilon - \xi^-_i - y_i \leqslant 0 &amp; &amp;(5) \\ &amp; y_i - \boldsymbol{w} \cdot \boldsymbol{x_i} - b - \epsilon - \xi^+_i \leqslant 0 &amp; &amp; (6) \\ &amp; -\xi^+_i \leqslant 0 &amp; &amp; (7) \\ &amp; -\xi^-_i \leqslant 0 &amp; &amp; (8) \\ &amp; \alpha_i^+,\alpha_i^-,\mu^+_i ,\mu^-_i \geqslant 0 &amp; &amp; (9) \\ \end{aligned} \tag{11} \right. \] 3.3.3. \(\alpha_i^+\)与\(a_i^-\)约束关系 \(\sum_{i=1}^m（\alpha_i^+ - \alpha_i^- ）＝ 0\) 式(8).2 \(0 \leqslant \alpha_i^+ ，\alpha_i^- \leqslant C\) 证明： \[ \left. \begin{aligned} 式(8).3：&amp; \ \ C - \alpha_i^+ - \mu_i^+ = 0 \\ (11).9：&amp; \ \ \mu_i^+ \geqslant 0， \alpha_i^+ \geqslant 0 \end{aligned} \right\} \Longrightarrow 0 \leqslant \alpha_i^+ \leqslant C \\ \\ 同理：0 \leqslant \alpha_i^- \leqslant C \] \(\alpha_i^+ \alpha_i^- = 0\) 证明： 假设\(\alpha_i^+ &gt; 0\)，\(\alpha_i^- &gt; 0\)，则根据式(11)中的1，2，5，6可得 \[ \begin{aligned} \boldsymbol{w} \cdot \boldsymbol{x_i} + b - \epsilon - \xi^-_i - y_i = 0 &amp; \ \ &amp;(1)\\ y_i - \boldsymbol{w} \cdot \boldsymbol{x_i} - b - \epsilon - \xi^+_i = 0 &amp; &amp;(2) \end{aligned} \tag{13} \] 以上两式相加可得：\(-2 \epsilon = \xi_i^+ + \xi_i^-\)，与约束\(\epsilon &gt; 0，\xi_i^+ \leqslant 0， \xi_i^- \leqslant 0\)相违背。 则证明\(\alpha_i^+\)和\(\alpha_i^-\)至少有一个等于0，即\(\alpha_i^+ \alpha_i^- = 0\)得证 3.3.4. 考虑约束项的拉格朗日对偶问题 \[ \left\{ \begin{aligned} obj ： &amp; \left. \begin{aligned} \max_{\boldsymbol{a^+},\boldsymbol{a^-}} \ \ &amp; -\frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m(a_i^- - a_i^+)(a_j^- - a_j^+)\boldsymbol{x}_i \cdot \boldsymbol{x}_j \\ &amp; - \epsilon\sum_{i=1}^m (a_i^- + a_i^+) - y_i \sum_{i=1}^m(a_i^- - a_i^+) &amp; \end{aligned} \right. \\ st ：&amp; \sum_{i=1}^m (\alpha_i^+ - a_i^-) ＝ 0 \\ &amp; 0 \leqslant \alpha_i^+， \alpha_i^- \leqslant C \\ &amp; \alpha_i^+ \alpha_i^- = 0 \end{aligned} \right. \tag{14} \] 3.4. 拉格朗日乘子、松弛因子、支持向量的关系 \(\xi_i^+ \xi_i^- = 0\) 一个样本的松弛因子最多只有一个不为0 考虑以下KKT条件： \[ \left\{ \begin{aligned} 式(8).3：&amp; C - \alpha_i^+ - \mu_i^+ = 0 \\ 式(11).3：&amp; \mu_i^+ \xi_i^+ = 0 \\ 式(12).2：&amp; 0 \leqslant \alpha_i^+ \leqslant C \\ \end{aligned} \right. \] 则可以得出以下结论： 当\(\alpha_i^+ = C\)时： \(\alpha_i^- = 0，\mu_i^+ = 0，\xi_i^+ &gt; 0\)，样本点在epsilon回归带之外，且在回归带上方 当0 &lt; \(\alpha_i^+ &lt; C\)时： \(\alpha_i^- = 0，\mu_i^+ &gt; 0， \xi_i^+ = 0\)，样本点在回归线及上epsilon回归带中间，为支持向量 当\(\alpha_i^+ = 0\)时： \(\mu_i^+ &gt; 0，\xi_i^+ = 0\)，样本点在回归线及上epsilon回归带中间 同理 当\(\alpha_i^- = C\)时： \(\alpha_i^+ = 0，\mu_i^- = 0，\xi_i^- &gt; 0\)，样本点在epsilon回归带之外，且在回归带下方 当0 &lt; \(\alpha_i^- &lt; C\)时： \(\alpha_i^+ = 0，\mu_i^- &gt; 0， \xi_i^- = 0\)，样本点在回归线及下epsilon回归带中间，为支持向量 当\(\alpha_i^- = 0\)时： \(\mu_i^- &gt; 0，\xi_i^- = 0\)，样本点在回归线及下epsilon回归带中间 考虑式(8).2 可以得到回归直线为： \[f(x) = \sum_{i=1}^m(a_i^+ - a_i^-)\boldsymbol{x_i} \cdot \boldsymbol{x} + b^* \tag{15}\] 满足\(a_i^+ - a_i^- \neq 0\)对应的点\(\boldsymbol{x_i}\)为支持向量 3.5. \(b^*\)的求解 当 \(0 \leqslant a_j^+ \leqslant C\)时，\(a_j^- = 0\)，由式(13).1可得 \[ b^*=-\sum_{i=1}^m(a_i^+ - a_i^-)\boldsymbol{x_i} \cdot \boldsymbol{x_j}+ \epsilon + y_j \] 当 \(0 \leqslant a_j^- \leqslant C\)时，\(a_j^+ = 0\)，由式(13).2可得 \[ b^*=-\sum_{i=1}^m(a_i^+ - a_i^-)\boldsymbol{x_i} \cdot \boldsymbol{x_j} - \epsilon + y_j \] 考虑当\(0 \leqslant a_j^+ \leqslant C\)时，\(a_j^- = 0\)，由式(13).1，当\(0 \leqslant a_j^- \leqslant C\)时，\(a_j^+ = 0\)，由式(13).2可得 \[ b^* = \left\{ \begin{aligned} &amp;-\sum_{i=1}^m(a_i^+ - a_i^-)\boldsymbol{x_i} \cdot \boldsymbol{x_j}+ \epsilon + y_j，0 \leqslant a_j^+ \leqslant C \\ &amp;-\sum_{i=1}^m(a_i^+ - a_i^-)\boldsymbol{x_i} \cdot \boldsymbol{x_j} - \epsilon + y_j，0 \leqslant a_j^- \leqslant C \end{aligned} \right. \] 4. 引入核函数. \(\kappa(\boldsymbol{x}, \boldsymbol{z}) = \varphi( \boldsymbol{x}) \cdot \varphi( \boldsymbol{z})\)替换以上\(\boldsymbol{x} \cdot \boldsymbol{z}\)形式 则： \[f(x) = \sum_{i=1}^m(a_i^+ - a_i^-)\kappa(\boldsymbol{x_i}, \boldsymbol{x}) + b^*\] \[ b^* = \left\{ \begin{aligned} &amp;-\sum_{i=1}^m(a_i^+ - a_i^-) \kappa(\boldsymbol{x_i}, \boldsymbol{x_j}) + \epsilon + y_j，0 \leqslant a_j^+ \leqslant C \\ &amp;-\sum_{i=1}^m(a_i^+ - a_i^-) \kappa(\boldsymbol{x_i}, \boldsymbol{x_j}) - \epsilon + y_j，0 \leqslant a_j^- \leqslant C \end{aligned} \right. \] 5. 参考资料 [1]《机器学习》，周志华著，2016 [2]《Machine Learning - A Probabilistic Perspective》，Kevin P. Murphy ，2012 [3]《Pattern Recognition And Machine Learning》，Christopher Bishop，2007 [4] 支持向量回归：http://blog.jasonding.top/2015/05/01/Machine%20Learning/【机器学习基础】支持向量回归]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime使用过程中的tips]]></title>
    <url>%2F2017%2F03%2F22%2Fsublime%2F</url>
    <content type="text"><![CDATA[测试环境：Mac 1. 为Sublime添加自动输入日期插件 2017-03-23 18:17:45 来源：http://www.phperz.com/article/14/1125/37633.html 1. 创建插件：Tools → Developer → New Plugin 替换代码为： 123456789import datetimeimport sublime_pluginclass AddCurrentTimeCommand(sublime_plugin.TextCommand): def run(self, edit): self.view.run_command("insert_snippet", &#123; "contents": "%s" % datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S") &#125; ) 保存为：addCurrentTime.py 创建快捷键：Preference → Key Bindings - User: 12345678[ &#123; "command": "add_current_time", "keys": [ "ctrl+shift+." ] &#125;] 保存。“ctrl+shift+.”为自动插入日期快捷键 2. Sublime标记修改但未提交到git的行 2017-03-22 18:18:10 来源：https://github.com/gornostal/Modific 1Cmd+Shift+P -&gt; Package Control: Install Package -&gt; Input: &quot;Modific&quot; -&gt; install...]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo使用过程中遇到的问题]]></title>
    <url>%2F2017%2F03%2F21%2Fhexo%2F</url>
    <content type="text"><![CDATA[Hexo 1. kill占用4000端口的程序 2017-03-29 16:09:39 1lsof -i tcp:4000 | awk '&#123;print $2&#125;' | xargs kill -9 2. Hexo 数学公式问题 2017-03-23 18:32:41 配置主题目录下的_config.yml的开关mathja 1enable: true 问题: 编辑公式是在latex编辑器里编辑正确的公式在hexo不能被正确的渲染 解决: 方案2(建议):更换Hexo的markdown渲染引擎 来源 在Hexo中渲染MathJax数学公式，http://www.jianshu.com/p/7ab21c7f0674 卸载后重新安装: 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-pandoc --save 执行以上命令配置生效，hexo clean -&gt; hexo s 方案1(麻烦):在不该配置文件的情况下，用二分法在’_‘、’[‘、’(‘、’{‘、’*‘、’'等处加转义尝试，如’_’ 3. Hexo本地测试与发布到git 本地测试: 1hexo s 必要时 1hexo clean 编译并发布到github: 1hexo d -g 4. Hexo正文中标题自动编号 参考：为Hexo博客标题自动添加序号 hexo-heading-index，http://www.tuicool.com/articles/7BnIVnI 1， 安装hexo-heading-index 1npm install hexo-heading-index --save 2， 修改顶层_config.yml 123456heading_index:enable: trueindex_styles: &quot;&#123;1&#125; &#123;1&#125; &#123;1&#125; &#123;1&#125; &#123;1&#125; &#123;1&#125;&quot;connector: &quot;.&quot;global_prefix: &quot;&quot;global_suffix: &quot;. &quot; 3， 修改Hexo主题下的_config.yml， 避免侧边栏重复自动生成编号，禁用侧边栏自动编号 123456# Table Of Contents in the Sidebartoc: enable: true # Automatically add list number to toc. number: false]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM]]></title>
    <url>%2F2017%2F03%2F20%2Fsvm%2F</url>
    <content type="text"><![CDATA[SVM，Support Vector Mechines，支持向量机 1. 线性可分SVM （图1， 线性支持向量机示意图） 1.1. 问题引出 有样本集 \(D=\{(\boldsymbol{x_1}，y_1)，(\boldsymbol{x_2}，y_2)，\ldots，(\boldsymbol{x_m}，y_m)\}\)。其中， \(\boldsymbol{x_i}\)是一个样本，列向量；标签\(y_i\in\{-1， 1\}\)且\(D\)是线性可分的。 找到一个最优超平面\(\boldsymbol{w}^T\boldsymbol{x} + b = 0\)(\(\boldsymbol{w}\)为是超平面系数，列向量；\(b\)为截距；\(\boldsymbol{x}\)为列向量)，使得在保证正确分类的情况下，样本点到超平面的最小距离最大化 最小距离：所以样本点到超平面的距离 支持向量：每个类别中到超平面距离最小的点 最大化：使得两类支持向量到超平面最小距离最大化，即两类支持向量点到超平面的距离相等 如上图: \(\boldsymbol{w}^T\boldsymbol{x} + b = 0\): 分离超平面 \(\boldsymbol{w}^T\boldsymbol{x} + b = 1\): 正例支撑超平面 \(\boldsymbol{w}^T\boldsymbol{x} + b = -1\): 负例支撑超平面 1.2. 问题的数学表达 \[假设条件(正确分类条件)\left\{ \begin{aligned} \frac{\boldsymbol{w}^T\boldsymbol{x_i} + b}{\lVert\boldsymbol{w}\rVert} &gt; 0， &amp; \ y_i = +1 \\ \frac{\boldsymbol{w}^T\boldsymbol{x_i} + b}{\lVert\boldsymbol{w}\rVert} &lt; 0， &amp; \ y_i = -1 \\ \end{aligned} \right. \] \[目标函数与约束条件(最大间隔条件)\left\{ \begin{aligned} obj: &amp; \max_{\boldsymbol{w}，b} \frac{\left|\boldsymbol{w}^T\boldsymbol{x_0} + b\right|}{\lVert\boldsymbol{w}\rVert} &amp; &amp; (支撑向量到超平面的距离最大) \\ st: &amp; \frac{\left|\boldsymbol{w}^T\boldsymbol{x_i} + b\right|}{\lVert\boldsymbol{w}\rVert} \geqslant \frac{\left|\boldsymbol{w}^T\boldsymbol{x_0}+ b\right|}{\lVert \boldsymbol{w}\rVert} ，\ i\in[1, m]&amp; &amp; (任意点到对应超平面的距离大于等于支撑向量到超平面的距离) \\ &amp; \boldsymbol{x_0}代表所有支撑向量 &amp; \end{aligned} \right. \] 点到直线的距离: 二维空间中，点\((x_0， y_0)\)到直线\(Ax+By+C=0\)的距离\(d=\frac{\left|Ax_0+By_0+C\right|}{\sqrt{A^{2}+B^{2}}}\). 多纬空间中，点\(\boldsymbol{x_0}\)到\(\boldsymbol{w}^T\boldsymbol{x} + b = 0\)的距离 \(d=\frac{\left| \boldsymbol{w}^T\boldsymbol{x_0} + b \right|}{\lVert \boldsymbol{w} \rVert_2}\) 1，由于\(\lVert\boldsymbol{w}\rVert &gt; 0\)， 则假设条件可以写成 \(y_i(\boldsymbol{w}^T\boldsymbol{x_i} + b) &gt; 0\) 2，由于超平面\(\boldsymbol{w}^T \boldsymbol{x} + b = 0\) 有无穷多个等价超平面 \(\kappa(\boldsymbol{w}^T\boldsymbol{x} + b) = 0\)， 所以存在等价超平面 \(\boldsymbol{w&#39;}^T\boldsymbol{x} + b&#39; = 0\)使得不在该超平面上的点\(x_0\)满足\(\left|\boldsymbol{w&#39;}^T\boldsymbol{x_0} + b&#39;\right| = 1\) 3，固定点到所有等价超平面的距离相等 基于以上，合并正确分类条件和最大间隔条件 \[\left\{ \begin{aligned} obj: &amp; \max_{\boldsymbol{w&#39;}，b&#39;} \frac{1}{\lVert\boldsymbol{w&#39;}\rVert} &amp; \\ st: &amp; y_i(\boldsymbol{w&#39;}^T\boldsymbol{x_i} + b&#39;)\geqslant 1，\ i\in[1， m]&amp; \\ &amp; 满足y_0(\boldsymbol{w&#39;}^T\boldsymbol{x_0} + b&#39;) = 1 的点为支撑向量&amp; \end{aligned} \right. \] 对上式变量替换及极值等价为以下: \[\left\{ \begin{aligned} obj: &amp; \min_{\boldsymbol{w}，b} \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 &amp; \\ st: &amp; y_i(\boldsymbol{w}^T\boldsymbol{x_i} + b)\geqslant 1，\ i\in[1， m]&amp; \\ &amp; 满足y_0(\boldsymbol{w}^T\boldsymbol{x_0} + b) = 1 的点为支撑向量&amp; \end{aligned} \right. \] 注:\(\ \)此时所求\(\boldsymbol{w}^T\boldsymbol{x} + b = 0\) 为原始假设超平面的等价超平面 问题目标： 找到满足上是约束的分离超平面参数\(\boldsymbol{w^*}\), \(b^*\)，求的分离超平面为\(\boldsymbol{w^*} \cdot \boldsymbol{x} + b^* = 0\), 对于待预测样本\(\boldsymbol{x_i}\)分类决策函数为：\[f(x) = sign(\boldsymbol{w^*} \cdot \boldsymbol{x_i} + b)\] 1.3. 求解 根据凸优化简介，原问题可以写作： \[\left\{ \begin{aligned} obj: &amp; \min_{\boldsymbol{w}，b} \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 &amp; \\ st: &amp; 1 - y_i(\boldsymbol{w}^T\boldsymbol{x_i} + b)\leqslant 0，\ i\in[1， m]&amp; \end{aligned} \right. \] 由于目标函数是二次函数是凸的，约束函数是关于\(w_i\) 和 \(b\)的仿射函数，所以此问题的为凸优化问题 引入拉格朗日乘子\(\alpha_i, i \in [1, \ldots, m]\)， 其拉格朗日函数为 \[L(\boldsymbol{w}, b, \boldsymbol{\alpha}) = \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 + \sum_{i=1}^m \alpha_i \big[1 - y_i(\boldsymbol{w}^T\boldsymbol{x_i} + b) \big]\] 1.3.1. 问题的拉格朗日函数 \[\min_{\boldsymbol{w}, b} \max_\boldsymbol{\alpha} L(\boldsymbol{w}, b, \boldsymbol{\alpha}) \] 1.3.2. 对偶函数为 \[\max_\boldsymbol{\alpha} \min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \boldsymbol{\alpha}) \] 1.3.3. KKT条件 对照凸优化简介，为有若干个不等式约束的凸优化情况 \[\left\{ \begin{aligned} &amp; \nabla_{\boldsymbol{x}} L = 0 &amp; \Longrightarrow &amp; \ \ \left\{ \begin{aligned} &amp; \nabla_{\boldsymbol{w}} L(\boldsymbol{w}, b, \boldsymbol{\alpha})_{\boldsymbol{w}=\boldsymbol{w}^*, b=b^*} = \boldsymbol{0} &amp; \ \ \ \ (1)\\ &amp; \nabla_{b} L(\boldsymbol{w}, b, \boldsymbol{\alpha})_{\boldsymbol{w}=\boldsymbol{w}^*, b=b^*} = 0 &amp; \ \ \ \ (2) \end{aligned} \right. \\ &amp; \lambda_i^*f_i(x^*) = 0 &amp; \Longrightarrow &amp; \ \ \alpha_i^* \big(y_i(\boldsymbol{w}^* \cdot \boldsymbol{x_i} + b^*) -1 \big) = 0 ， i = 1, \ldots, m &amp; \ \ \ \ (3)\\ &amp; f_i(x) \leqslant 0 &amp; \Longrightarrow &amp; \ \ 1 - y_i(\boldsymbol{w}^* \cdot \boldsymbol{x_i} + b^*) \leqslant 0， i = 1, \ldots, m &amp; \ \ \ \ (4) \\ &amp; \lambda_i \geqslant 0 &amp; \Longrightarrow &amp; \ \ \alpha_i \geqslant 0， i = 1, \ldots, m &amp; \ \ \ \ (5) \end{aligned} \right. \] 说明：(1)，(2)式中，参数\(\boldsymbol{w}, b\)对应凸优化简介中KKT条件的\(\boldsymbol{x}\)， 是待求参数。\(y_i(\boldsymbol{w}^T\boldsymbol{x_i} + b)\geqslant 1，\ i\in[1， m]\) 约束中的\(\boldsymbol{x_i}\)和\(y_i\)为一个样本及其对应的类别，为已知常数 1.3.4. \(\min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \boldsymbol{\alpha})\)的求解 由KKT条件(1)可得： \[\boldsymbol{w^*} - \sum_{i=1}^m \alpha_i y_i \boldsymbol{x_i} = \boldsymbol{0} \Longrightarrow \boldsymbol{w^*} = \sum_{i=1}^m \alpha_i y_i \boldsymbol{x_i} \ \ \ \ (6)\] 由KKT条件(2)可得： \[\sum_{i=1}^m \alpha_i y_i = 0 \ \ \ \ (7)\] 由KKT条件(3)，(4)，(5)联立可得： 当\(a_i^* &gt; 0\)时，其对应不等式约束为为边界条件，对应样本点为支持向量，有： \[\begin{aligned} &amp; y_j(\boldsymbol{w}^* \cdot \boldsymbol{x_j} + b^*) -1 = 0， i = 1, \ldots, n。\ \ ( 其中n为支撑向量个数) \\ &amp; 将(6)式带入上式，且 1 = y_j \cdot y_j \\ &amp; \therefore b^* = y_j - \sum_{i=i}^m \alpha_i ^ * y_i(\boldsymbol{x_i} \cdot \boldsymbol{x_j}) \end{aligned} \] 当\(a_i^* = 0\)时，其对应的不等式约束\(y_i (\boldsymbol{w^*} \cdot \boldsymbol{x_i} + b^*) -1 &lt; 0, i = 1,\ldots, m\)为非边界条件，该样本不在支撑超平面 \[ \begin{aligned} &amp; \min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \boldsymbol{\alpha}) \\ = &amp; \frac{1}{2}\lVert\boldsymbol{w^*}\rVert^2 + \sum_{i=1}^m \alpha_i \big(1 - y_i(\boldsymbol{w^*} \cdot \boldsymbol{x_i} + b^*)\big) \\ = &amp; \frac{1}{2}\lVert\boldsymbol{w^*} \rVert^2 + \sum_{i=1}^m \alpha_i - \boldsymbol{w^*} \cdot \sum_{i=1}^m \alpha_i y_i \boldsymbol{x_i} - \sum_{i=1}^m \alpha_i y_i b^* \\ \ = &amp; \frac{1}{2}\lVert\boldsymbol{w^*} \rVert^2 + \sum_{i=1}^m \alpha_i - \boldsymbol{w^*} \cdot \boldsymbol{w} - \sum_{i=1}^m \alpha_i y_i b^* \ \ \ \because 式(6)： \boldsymbol{w^*} = \sum_{i=1}^m \alpha_i y_i \boldsymbol{x_i}：\\ = &amp; -\frac{1}{2}\lVert\boldsymbol{w^*} \rVert^2 + \sum_{i=1}^m \alpha_i - b^* \sum_{i=1}^m \alpha_i y_i \\ = &amp; - \frac{1}{2} \sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j(\boldsymbol{x_i} \cdot \boldsymbol{x_j}) + \sum_{i=1}^m \alpha_i \ \ \ \ \because 式(6)， 式(7)： \sum_{i=1}^m \alpha_i y_i = 0 \end{aligned} \] 1.3.5. \(\max_\boldsymbol{\alpha} \min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \boldsymbol{\alpha})\) 函数的解 考虑KKT条件中上式仍有约束的条件，有如下： \[ \begin{aligned} &amp; \max_\boldsymbol{\alpha} \min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \boldsymbol{\alpha}) \\ = &amp; \left\{ \begin{aligned} obj\ :\ &amp;\max_{\boldsymbol{\alpha}} -\frac{1}{2} \sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j(\boldsymbol{x_i} \cdot \boldsymbol{x_j}) + \sum_{i=1}^m \alpha_i, \ \ \ \ i，j \in [1,\ldots, m]\\ st\ :\ &amp;\ \sum_{i=1}^m \alpha^* y_i = 0，i \in [1,\ldots, m] \\ &amp;\ \alpha_i \geqslant 0， i \in [1,\ldots, m] \end{aligned} \right. \end{aligned} \] 上式可以通过SMO方法求得 求得\(a_i^*\)后，分离超平面为： \[\sum_{i=1}^m a_i^* y_i(\boldsymbol{x} \cdot \boldsymbol{x_i}) + b^* = 0， (其中(\boldsymbol{x_i}, y_i）为训练样本及对应标签)\] 分类决策函数为： \[f(x) = sign \big[\sum_{i=1}^m a_i^* y_i(\boldsymbol{x_i} \cdot \boldsymbol{x}) + b^* \big] \ \ \ (\boldsymbol{x} 为待预测样本)\] 线性可分支持向量机有且只有一个最优解, 具体证明见《统计学习方法》- 李航 2. 线性不可分SVM 2.1. 问题表示及拉格朗日函数 对每个样本点引入一个松弛变量\(\xi_i \geqslant 0\)，使得线性不可分的点对应的支持超平面向着平行于其法线且朝着分离超平面的方向移动$ $作为伪支撑超平面上为参照（该点在此伪支撑超平面上）， 使得函数距离约束变松。同时对松弛因子加入惩罚系数（超参数C， C &gt; 0）。以上被称为软件个最大化，数学描述为： \[\left\{ \begin{aligned} obj: \ &amp; \min_{\boldsymbol{w},b,\boldsymbol{\xi}} \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 + C \sum_{i=1}^m \xi_i， \ C \geqslant 0 , &amp; i\in[1， m] &amp; \\ st: \ &amp; y_i(\boldsymbol{w}^T\boldsymbol{x_i} + b)\geqslant 1 - \xi_i, &amp; i\in[1， m]&amp; \\ &amp; \xi_i \geqslant 0 , &amp;\ i\in[1， m]&amp; \\ &amp; 满足y_0(\boldsymbol{w}^T\boldsymbol{x_0} + b) = 1 的点为支撑向量 &amp; \end{aligned} \right. \tag{2.1} \] 为松弛变量\(\boldsymbol{\xi}\)引入拉格朗日乘子\(\boldsymbol{\mu}\)，其拉格朗日函数为： \[ \begin{aligned} &amp; L(\boldsymbol{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu}) \\ = &amp; \frac{1}{2}\lVert\boldsymbol{w}\rVert^2 + C \sum_{i=1}^m \xi_i + \sum_{i=1}^m \alpha_i \big[ 1 - \xi_i - y_i(\boldsymbol{w}^T\boldsymbol{x_i} + b) \big] - \sum_{i=1}^m \mu_i \xi_i \end{aligned} \] 2.2. KKT条件 \[\left\{ \begin{aligned} &amp; \nabla_{\boldsymbol{w}} L(\boldsymbol{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu})\ = \boldsymbol{0} \Longrightarrow w^* - \sum_{i=1}^m \alpha_i^* y_i x_i =0 &amp; \ \ \ \ (1)\\ &amp; \nabla_{b} L(\boldsymbol{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu}) = 0 \Longrightarrow -\sum_{i=1}^m a_i^* y_i = 0 &amp; \ \ \ \ (2) \\ &amp; \nabla_{\xi_i} L(\boldsymbol{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu}) = 0 \Longrightarrow C - a^*_i - u^*_i = 0 \ &amp; i\in[1， m] \ \ \ \ (3) &amp;\\ &amp; y_i(\boldsymbol{w^*} \cdot \boldsymbol{x_i} + b^*)\geqslant 1 - \xi_i^*, &amp; i\in[1， m] \ \ \ \ (4)&amp; \\ &amp; \xi_i^* \geqslant 0 ， &amp;\ i\in[1， m] \ \ \ \ (5)&amp; \\ &amp; \alpha_i^* \geqslant 0 ， &amp;\ i\in[1， m] \ \ \ \ (6) &amp; \\ &amp; \mu_i^* \geqslant 0 ， &amp;\ i\in[1， m] \ \ \ \ (7) &amp; \\ &amp; \alpha_i^* \big[ 1 - \xi_i^* - y_i(\boldsymbol{w^*} \cdot \boldsymbol{x_i} + b^*) \big] = 0 ， &amp;\ i\in[1， m] \ \ \ \ (8)&amp; \\ &amp; \mu_i^* \xi_i^* = 0 ， &amp;\ i\in[1， m] \ \ \ \ (9) &amp; \\ \end{aligned} \tag{2.2} \right. \] 2.3. 求解 2.3.1. \(\min_{\boldsymbol{w}, b, \boldsymbol{\xi}} L(\boldsymbol{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu})\)的求解 将(1),(2),(3)带入拉格朗日函数得， \[ \begin{aligned} &amp; \min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu}) \\ = &amp; - \frac{1}{2} \sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j(\boldsymbol{x_i} \cdot \boldsymbol{x_j}) + \sum_{i=1}^m \alpha_i \end{aligned} \] 2.3.2. \(\max_{\boldsymbol{\alpha}, \boldsymbol{\mu}} \min_{\boldsymbol{w}, b, \boldsymbol{\xi}} L(\boldsymbol{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu})\) 考虑对偶问题最大化时求\(\boldsymbol{\alpha}\) 考虑KKT条件对上式有约束的条件： 由(3),(5),(7)消去\(\mu_i\)得： \[0 \leqslant \alpha_i \leqslant C\] 则， 对偶函数的可以表示为 \[ \begin{aligned} &amp; \max_\boldsymbol{\alpha} \min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \boldsymbol{\alpha}) \\ = &amp; \left\{ \begin{aligned} obj\ :\ &amp;\max_{\boldsymbol{\alpha}} -\frac{1}{2} \sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j(\boldsymbol{x_i} \cdot \boldsymbol{x_j}) + \sum_{i=1}^m \alpha_i, \ \ \ \ i，j \in [1,\ldots, m]\\ st\ :\ &amp;\ \sum_{i=1}^m \alpha^* y_i = 0，i \in [1,\ldots, m] \\ &amp;\ 0 \leqslant \alpha_i \leqslant C， i \in [1,\ldots, m] \end{aligned} \right. \end{aligned} \] 用SMO求得\(\boldsymbol{\alpha}\)，对于 \(0 &lt; a_i &lt; C\) 的样本点： 假设满足\(0 &lt; a_i &lt; C\)的个数为n \[b^j = y_j - \sum_{i=i}^m \alpha_i ^ * y_i(\boldsymbol{x_i} \cdot \boldsymbol{x_j})，j \in [1, n]\] 实际求解过程中\[b^*=\frac{1}{n} \sum_{j=1}^n b^j\] 分离超平面为： \[\sum_{i=1}^m a_i^* y_i(\boldsymbol{x} \cdot \boldsymbol{x_i}) + b^* = 0， (其中(\boldsymbol{x_i}, y_i)为训练样本及对应标签)\] 分类决策函数为： \[f(x) = sign \big[\sum_{i=1}^m a_i^* y_i(\boldsymbol{x_i} \cdot \boldsymbol{x}) + b^* \big] \ \ \ (\boldsymbol{x} 为待预测样本)\] 超参数\(C\)为正则化常数，\(C\)越大， 分离超平面和支持超平面越距离越近, 训练集的准确率越高, 模型泛化能里越弱 当惩罚系数\(C \to +\infty\) 时，退化为线性可分的情况 拉格朗日乘子、松弛因子与支撑向量之间的关系： 当\(\alpha_i^* = 0\)时：非支持向量 当\(\alpha_i^* = C\)时：支持向量 当\(\xi_i^* &gt; 1\)时： \(x_i\)为误分类点 当\(\xi_i^* = 1\)时： \(x_i\)为在分隔超平面上 当\(0 &lt; \xi^* &lt; 1\)时： \(x_i\)在分隔超平面和正确支撑超平面之间 当\(\xi^* = 0\)时：\(x_i\)是支撑超平面上的支持向量 \(0 &lt; \alpha_i^* &lt; C\)： \(x_i\)式支撑超平面上的支持向量。根据KKT条件\(C - a_i^* - u_i^* = 0\)，此时\(u_i^* &gt; 0\)， 另有KKT条件\(u_i^*\xi_i^* = 0\)，可得\(\xi_i^*=0\)，即样本\(\boldsymbol{x_i}\)在支撑超平面上 2.3.3. 非线性不可分SVM 2.3.3.1. 问题求解 对于线性不可分的情况，使用核函数将原特征映射到更高维度后进行分类 核函数表示为 \(\kappa(x, z) = \varphi(x) \cdot \varphi(z)\)，其中\(\varphi(x)\) 为某种映射函数 对于线性不可分的情况 \[ \begin{aligned} &amp; \max_\boldsymbol{\alpha} \min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \boldsymbol{\alpha}) \\ = &amp; \left\{ \begin{aligned} obj\ :\ &amp;\max_{\boldsymbol{\alpha}} -\frac{1}{2} \sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j \kappa(\boldsymbol{x_i}, \boldsymbol{x_j}) + \sum_{i=1}^m \alpha_i, \ \ \ \ i，j \in [1,\ldots, m]\\ st\ :\ &amp;\ \sum_{i=1}^m \alpha^* y_i = 0，i \in [1,\ldots, m] \\ &amp;\ 0 \leqslant \alpha_i \leqslant C， i \in [1,\ldots, m] \end{aligned} \right. \end{aligned} \] 求得\(a_i^*\)后 \[b^* = \frac{1}{n} \sum_{j=1}^n \big[y_j - \sum_{i=i}^m \alpha_i ^ * y_i \kappa(\boldsymbol{x_i}, \boldsymbol{x_j}) \big]， 0 &lt; a_j &lt; C\] 分离超平面为： \[\sum_{i=1}^m a_i^* y_i \kappa(\boldsymbol{x_i}, \boldsymbol{x_j}) + b^* = 0， (其中(\boldsymbol{x_i}, y_i）为训练样本及对应标签)\] 分类决策函数为： \[f(x) = sign \big[\sum_{i=1}^m a_i^* y_i \kappa(\boldsymbol{x_i}, \boldsymbol{x}) + b^* \big] \ \ \ (\boldsymbol{x} 为待预测样本)\] 2.3.4. 核函数 以下x,z为向量 线性核 \[\kappa(x, z) = x \cdot z\] 多项式核 \[\kappa(x, z) = (\gamma x \cdot z + r)^p\] rbf核（高斯核） \[\kappa(x, z) = exp \big(-\frac{\lVert x - z\rVert^2 }{2 \sigma^2}\big)\] 双曲正切核（sigmoid核） \[\kappa(x, z) = tanh(\gamma x \cdot z + r)\] 更多核函数见机器学习核函数手册 3. SMO Sequential minimal optimization 序列最小优化算法，https://en.wikipedia.org/wiki/Sequential_minimal_optimization 详见《统计学习方法》，李航著，\(P_{124}\) 4. 损失函数 考虑约束问题式(2.1)，将不等式约束部分和约束整合，可以得到线性支持向量机的损失为hinge损失，形式如下： \[\sum_{i=1}^m \max \left\{ 0, 1 - (\boldsymbol{w}^T x_i + b) y_i \right\}\] 当随时函数点样本点到正确分类的超平面的集合距离小于1时, 有损失, 损失为该点到对应正确分类支撑超平面的函数距离.即: + 对于正例(\(y_i = 1\)): 正支撑超平面的下方样本点有损失, hinge损失为点到正支撑超平面的函数距离 + 对于负例(\(y_i = -1\)): 负支撑超平面的上方样本点有损失, hinge损失为点到负支撑超平面的函数距离 （图2, hinge损失, 0-损失, Logistic损失） hige核心在于只关心核心点(支持向量)造成的损失,且这部分损失是线性的(区别与0-1损失),放弃一定范围外且正确分类点(区别于logistic损失) hinge损失 + L2正则的最小化问题等价为线性支持向量机的最优化问题.即支持向量最优化问题可以表示为如下: \[ \min\limits_{\boldsymbol{w}, b} \left[ \sum_{i=1}^m \max \left\{ 0, 1 - (\boldsymbol{w}^T x_i + b) y_i \right\} + \lambda \lVert \boldsymbol{w} \rVert^2 \right] \] \[\lambda=\frac{1}{2C},\ C &gt; 0\] 从hinge损失的角度考虑，因为全局不可导，所以可以用随机梯度下降的来求参数： 为方便推导， 每个样本扩充一个常数1， 则有\(x_i \to (x_i, 1)^T\)， 则 \(\boldsymbol{w}^T x_i + b \to \boldsymbol{w}^T x_i\) \[ \nabla_\boldsymbol{w} ＝ \left\{ \begin{aligned} &amp; 2 \lambda \boldsymbol{w} - \boldsymbol{x_i} y_i &amp;，&amp; if \ \ \boldsymbol{w}^T \boldsymbol{x_i} &lt; 1 \\ &amp; 2 \lambda \boldsymbol{w}&amp; ， &amp;other \\ \end{aligned} \right. \] 则学习率为第t次步长\(\eta_t\)的随机梯度下降算法为： \[ \boldsymbol{w_{t+1}} ＝ \left\{ \begin{aligned} &amp; \boldsymbol{w_t} - \eta_t (2 \lambda \boldsymbol{w_t} - \boldsymbol{x_i} y_i) &amp;，&amp; if \ \ \boldsymbol{w}^T \boldsymbol{x_i} &lt; 1 \\ &amp; \boldsymbol{w_t} - \eta_t (2 \lambda \boldsymbol{w_t})&amp; ，&amp; other \\ \end{aligned} \right. \] 对于SVM在只关注核心点的同时, 选择分离超平面的原则使得熵最大, 即使得两支撑超平面到分离超平面的距离相等,保证了其泛化能力, 比较适合稀疏样本分类。 但如果支持向量是个噪声点的话会对结果有较大的影响， 可以通过降低原约束问题的\(C\)降低对噪声点的敏感程度 （图3, 合页, hinge损失亦被称为合页损失) 5. SVM与感知机异同 感知机是误分类点算法驱动的, 其损失函数: \[\sum_{i=1}^m \max \left\{ 0, - (\boldsymbol{w}^T \boldsymbol{x_i} + b) y_i \right\}\] 最优化问题可以表示为 \[ \min\limits_{\boldsymbol{w}, b} \left[ \sum_{i=1}^m \max \left\{ 0, - (\boldsymbol{w}^T \boldsymbol{x_i} + b) y_i \right\} \right] \] 对比SVM的hinge + L2-norm, 可以看到SVM比感知机加了正确分类但函数间隔小于1的惩罚 ,并且假如L2-norm项 由于感知机对所有分类正确的点都没有惩罚, 所以即使线性可分的情况下,感知机模型也不是唯一的.感知机参数可以用随机梯度下降方法训练得到, 每次选择当前参数下误分类样本用于训练 6. 概率化的结果输出 考虑二分类问题, 对于样本类别\({+1, -1}\), 假设待预测样本的为\(+1\)概率\(p = \frac{1}{1+exp^{-\phi(x)}}\)，则其为\(-1\)的概率为1 - p, 则有 \(ln(\frac{p}{1-p}) = \phi(x)\), \(\phi(x)\)发生与不发生的概率比取对数, 被称为对数几率比 \[\left\{ \begin{aligned} &amp; \frac{p}{1-p} &gt; 1 \Rightarrow \phi(x) &gt; 0： \ \ &amp; 认为样本类别为 +1 \\ &amp; \frac{p}{1-p} &lt; 1 \Rightarrow \phi(x) &lt; 0： \ \ &amp; 认为样本类别为 -1 \\ \end{aligned} \right. \tag{6.1} \] 对于SVM，假设训练好参数\(\boldsymbol{w}\), \(b\). 令\(f(x) = \boldsymbol{w} \cdot \boldsymbol{x} + b\), 则有 \[\left\{ \begin{aligned} &amp; f(x) &gt; 0：&amp;认为样本类别为 +1 \\ &amp; f(x) &lt; 0：&amp;认为样本类别为 -1 \\ \end{aligned} \right. \tag{6.2} \] \[\left\{ \begin{aligned} &amp; f(x) &gt; 0：&amp;认为样本类别为 +1 \\ &amp; f(x) &lt; 0：&amp;认为样本类别为 -1 \\ \end{aligned} \right. \tag{6.2} \] 对比(6.1),(6.2)可以看出SVM的函数距离和对数几率比对预测结果有类似,所以假设SVM的函数距离的线性函数是某种对数几率比:即认为\(\phi(x) = -\big[af(x) + b \big]\), 最终概率 \[p = \frac{1}{1+exp^{f(x)}}\] 其中\(a\), \(b\)通过极大似然估计,在mlapp中特别说明如果用原来数据集求解\(a\),\(b\)容易过拟合, 建议用单独数据集训练 其极大似然为: \[L=\prod_{i=1}^m \big[\frac{1}{1+exp^{af(x_i) + b}}\big]^\frac{y_i+1}{2} \cdot \big[\frac{af(x_i) + b}{1+exp^{af(x_i) + b}}\big]^ {-\frac{y_i - 1}{2}}\] 对数极大似然问题为: \[l(a, b) =\sum_{i=1}^m \Bigg[\frac{y_i+1}{2} \frac{1}{1+exp^{af(x_i) + b}} -\frac{y_i - 1}{2} \frac{af(x_i) + b}{1+exp^{af(x_i) + b}}\Bigg]\] 所以问题的解如下: \[\arg \min_{a, b}-l(a, b)\] 可以通过梯度下降或拟牛顿等方法求得a, b Platt还提出把标签分别把正负例的y_i转化为和其类别样本数相关的概率数，转化方法见https://en.wikipedia.org/wiki/Platt_scaling ! 但是mlapp中指出验证效果并不理想, 同时相关向量机(Relevance Vector Machine)可以较好的拟合概率 7. 多分类SVM 7.1. 直接公式法 https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf 7.2. 一对剩余（One Versus Rest， OVR） 把其中一个类别做为\(＋1\)类， 其他类别做为\(－1\)类， 训练K个二分SVM，选择 \(y_i(\boldsymbol{w}^T x_i + b)\)的最大的对应的距离， 因为是一对多的情况所以不平衡样本的问题突出，可以通过正例重采样，Lee et al. (2001) 提出修改负例的标签为\(-\frac{1}{k-1}\),因为不是在同一参考下生成的模型，所以通过直接找出函数距离最大的作为最终分类可信度受限。 7.3. 一对剩余（One Versus One， OVO） 一次拿出两类训练分类器， k类一共有\(\frac{k(k-1)}{2}\)个分类器，对每个样本的类别结果做统计，众数对应的类别为最后对应的类别 8. SVR http://reset.pub/2017/03/26/svr 9. sklearn中的SVM http://scikit-learn.org/stable/modules/svm.html#svm SVC: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC http://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC SVR: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR 12345from sklearn import svmclf = svm.SVC() # 分类clf = svm.SVR() # 回归 分类问题中，针对不平衡样本， 通过class_weight参数，达到提高少数样本的召回率 10. 参考资料 [1]《统计学习方法》，李航著，2012 [2]《机器学习》，周志华著，2016 [3]《Machine Learning - A Probabilistic Perspective》，Kevin P. Murphy ，2012 [4]《Pattern Recognition And Machine Learning》，Christopher Bishop，2007 [5] 维基百科-支持向量机：https://zh.wikipedia.org/wiki/支持向量机 [6] 随机梯度求SVM http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf [7] 多分类SVM：https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf [8] 多分类SVM：[!PDF] Multi-Class Support Vector Machine - Springer [9] PRML翻译：https://mqshen.gitbooks.io/prml/ [10] 机器学习核函数手册：https://my.oschina.net/lfxu/blog/478928 [11] sklearn－核函数：http://scikit-learn.org/stable/modules/svm.html#svm-kernels [12] 支持向量回归：http://blog.jasonding.top/2015/05/01/Machine%20Learning/【机器学习基础】支持向量回归 [13] SVM等于Hinge损失 + L2正则化：http://breezedeus.github.io/2015/07/12/breezedeus-svm-is-hingeloss-with-l2regularization.html [14] 维基百科－Platt_scaling： https://en.wikipedia.org/wiki/Platt_scaling [15] sklearn－svm：http://scikit-learn.org/stable/modules/svm.html#svm]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[凸优化简介]]></title>
    <url>%2F2017%2F03%2F18%2Fconvex-optimization%2F</url>
    <content type="text"><![CDATA[Convex Optimization，凸优化 SVM中的凸优化中的KKT 1. 几个基本概念 1.1. 凸集 如果集合\(C\)中任意两点间的线段仍然在\(C\)中，即满足: \[\left. \begin{aligned} \forall x_1, x_2 \in C &amp; \\ \forall\ \theta\ ,\ 0 \leqslant \theta \leqslant 1 &amp; \end{aligned} \right\} \Longrightarrow \theta x_1 + (1-\theta)x_2 \in C \] 多点推广: \[\left. \begin{aligned} \forall x_1, x_2, \ldots, x_k \in C &amp; \\ \theta_i \geqslant 0, \ i = 1, \ldots, k &amp; \\ \sum_{i=1}^k \theta_i = 1 &amp; \end{aligned} \right\} \Longrightarrow \theta_i x_i + \ldots + \theta_k x_k \in C \] 多点推广的证明: 1, 三点成立证明 \[\begin{aligned} \left. \begin{aligned} \left. \begin{aligned} \left. \begin{aligned} \forall x_1, x_2 \in C &amp; \\ \forall\ \theta_1\ ,\ 0 \leqslant \theta_1 \leqslant 1 &amp; \end{aligned} \right\} \Longrightarrow \theta_1 x_1 + (1-\theta_1)x_2 \in C \\ \forall\ \theta_2\ ,\ 0 \leqslant \theta_2 \leqslant 1 \\\ \forall\ x_3 \in C \\ \end{aligned} \right\} \Longrightarrow \theta_2 (\theta_1 x_1 + (1-\theta_1)x_2) + (1-\theta_2)x_3 \in C \\ \left. \begin{aligned} &amp;0 \leqslant \theta_1, \theta_2, \theta_3 \leqslant 1 \\ set:\ &amp;\theta_1&#39;=\theta_2 \theta_1 \\ &amp;\theta_2&#39;=\theta_2(1-\theta_1) \\ &amp;\theta_3&#39;=(1-\theta_2) \end{aligned} \right\} \Longrightarrow \left\{ \begin{aligned} \theta_1&#39; + \theta_2&#39; + \theta_3&#39; = 1 \\ 0 \leqslant \theta_1&#39;,\theta_2&#39;, \theta_3&#39; \leqslant 1 &amp; \end{aligned} \right. \end{aligned} \right\} &amp;\\ \\ \Longrightarrow \theta_1&#39; x_1 + \theta_2&#39; x_2 + \theta_3&#39; x_3 \in C &amp; \end{aligned} \] 整理上式得: \[\left. \begin{aligned} \forall \ x_1, x_2, x_3 \in C &amp; \\ \forall \ \theta_i, 0 \leqslant \theta_i \leqslant 1 &amp; \ \ i \in [1, 3]\\ \theta_1&#39; + \theta_2&#39; + \theta_3&#39; = 1&amp; \end{aligned} \right\} \Longrightarrow \theta_1&#39; x_1 + \theta_2&#39; x_2 + \theta_3&#39; x_3 \in C \] 2, k点成立正证明: 用同样的方法可以证明k-1个点到k个点的情况, 即通过归纳法证得多点也成立 1.2. 仿射集 如果集合\(C\)中任意两点决定的直线仍然在\(C\)中, 即满足: \[\left. \begin{aligned} \forall x_1, x_2 \in C &amp; \\ \forall\ \theta\ &amp; \end{aligned} \right\} \Longrightarrow \theta x_i + (1-\theta)x_2 \in C \] 类似凸集可推广到多点 凸集是仿射集的真子集 1.3. 凸函数 如果函数\(\ f\ \)定义域是凸集，对于定义域内的任意\(\ x, y \in \boldsymbol{dom}\ f\ \) 和任意\(\ 0 \leqslant \theta \leqslant 1\ \), 有 \(f\big(\theta x + (1-\theta)y\big) \leqslant \theta f(x) + (1-\theta)f(y)\), 即: \[\left. \begin{aligned} \ 函数f\ 的定义域\boldsymbol{dom}\ f\ 是凸集 &amp; \\ \ \forall x, y \in \boldsymbol{dom}\ f\ &amp; \\ \forall \theta\ ,\ 0 \leqslant \theta \leqslant 1 &amp; \end{aligned} \right\} \Longrightarrow f(\theta x + (1-\theta)y) \leqslant \theta f(x) + (1-\theta)f(y) \] 即，定义域内，两点的期望的函数值小于分别求函数值的期望， 亦即\(\ f(E(x)) \leqslant E(f(x))\) 当以上不等式存在’\(=\)’成立时，是非严格凸， 否则为严格凸 1.3.1. 凸函数的判断 1.3.1.1. 根据定义判断 直接利用定义证明 1.3.1.2. 一阶条件判断 假设函数\(\ f\ \)可微， 则: \[f是凸函数 \Longleftrightarrow \left\{ \begin{aligned} &amp; \boldsymbol{dom} \ f\ 是凸集 \\ &amp; \forall \ x, y \in \boldsymbol{dom}\ f \\ &amp; f(y) \geqslant f(x) + \nabla f(x)^\mathsf{T}(y-x) \ \ \ \ \ (任意点的函数值大于等于任意切线在该点的值) \end{aligned} \right. \] 1.3.1.3. 二阶条件判断 如果函数\(\ f \ \)二阶可微， 则\(\ f \ \)为凸函数的充要条件是， 二阶导数\(Hessian\)矩阵是正定阵 1.4. 仿射函数 如果具有 \(f(x)\ =\ \boldsymbol{A}\boldsymbol{x}+\boldsymbol{b}\)的形式的函数\(\ f\ \)称为仿射函数 凸函数是仿射函数的真子集 2. 优化与凸优化问题表示 2.1. 优化问题的一般形式 \[\begin{aligned} obj\ :&amp;\ \ \min\ f_0(x) \\ st\ :&amp;\ \ f_i(x) \leqslant 0, \ i = 1,2, \ldots, m \ \\ &amp;\ \ h_i(x) = 0, i = 1, 2, \ldots, p \end{aligned} \] 2.2. 凸优化问题的一般形式 \[\begin{aligned} obj\ :&amp;\ \ \min\ f_0(x) &amp; \\ st\ :&amp;\ \ f_i(x) \leqslant 0\ , \ i = 1,2, \ldots, m &amp;\\ &amp;\ \ h_i(x) = 0, i = 1, 2, \ldots, p &amp; \\ &amp;\ \ f_0(x)\ 是凸函数 \\ &amp;\ \ f_i(x)\ 是凸函数，\ i \in [1, m] &amp; \\ &amp;\ \ h_i(x)\ 是仿射函数， 即h_i(x)为\boldsymbol{A}\boldsymbol{x}+\boldsymbol{b} 的形式\ , i \in [1, p] &amp; \end{aligned} \] 3. 拉格朗日乘子法 3.1. 广义拉格朗日函数: \[\begin{aligned} &amp; L(x, \boldsymbol{\lambda}, \boldsymbol{\nu}) = f_0(x) + \sum_{i=1}^m\lambda_i f_i(x) + \sum_{i=1}^p \nu_i h_i(x) \\ &amp; \boldsymbol{\nu} \in \boldsymbol{R}^p , \ \boldsymbol{\lambda} \in \boldsymbol{R}^m \\ &amp; \lambda_i \geqslant 0, \ i \in [1, m] \end{aligned}\] 3.2. 拉格朗日函数: \[\theta_p(x) = \max_{\boldsymbol{\lambda}, \boldsymbol{\nu}}{L(x, \boldsymbol{\lambda}, \boldsymbol{\nu})} \left\{ \begin{aligned} f_0(x), &amp; \ \ x满足原始问题的约束 \\ +\infty, &amp; \ \ 其他\\ \end{aligned} \right. \] 3.3. 拉格朗日函数的理解 (图1, \(\ f_0(x)\)等高线示与约束条件示意图,蓝色箭头方向为等高线对应值降低的方向,当\(g(x, y) - c = 0\)时红色线表示等式约束;当\(g(x, y) - c \leqslant 0\) 红色线箭头方向为不等式约束成立的方向) 等高线: 考虑三维情况, \(z=f_0(x, y)\)的情况，等高面\(z = d\)与函数\(f_0(x, y)\)相交的部分为等高线, 等高线在\((x, y)\)平面的投影可表示为\(f_0(x, y)=d\) 3.3.1. 一个等式的约束问题 如图假设等式约束为 \(h_1(x, y)=g(x, y) - c=0\)， 拉格朗日函数表示为: \(L=f_0(x, y) + \nu h_1(x, y)\). 如果函数\(f_0(x, y)\) 的值域连续, 则其等高线投影与\(g(x, y) -c\)相切处取得等式约束条件下的极值，此时\(f_0(x, y)\) \[\left. \begin{aligned} &amp; \left. \begin{aligned} f_0(x, y)与h_1(x, y)相切\Rightarrow \nabla_{x, y} f_0(x, y) = \nu&#39; (\nabla_{x, y} h_1(x, y)) \Rightarrow \nabla_{x, y}[f_0 + \nu(h_1)] =0 &amp; \\ h_1(x,y)=0 \Rightarrow \nabla_{\nu}[f_0 + \nu(h_1)] = 0 &amp; \end{aligned} \right\} \\ \\ &amp; \Longrightarrow \nabla_{x, y, \nu} \big[f_0(x) + \nu h_1(x) \big] = 0 \\ &amp; \Longrightarrow 目标函数f(x,y)在一个等式约束条件下取得最值处的解与L=f_(x, y) + \nu h_1(x,y)极值的解等价 \end{aligned} \right. \] - 注: \(\nu&#39;\) 可以看作是两个等价切线(超平面)的系数, 同分割超平面的等价超平面的系数\(k\) \(\nu = -\nu&#39;\) 3.3.2. 多个等式约束问题 (图2,等高线及含有两个约束条件的情况示意图) 多个等式约束看作在满足约束条件下集合处， 定义极值处的梯度方向为多个约束加权后得到的梯度和目标函数\(f_0(极点)\)处的梯度平行 即极值处满足: \[\left. \begin{aligned} &amp; \nabla_{\boldsymbol{x}} f(x) = \sum_{i=1}^p \lambda_i&#39; \nabla_{\boldsymbol{\boldsymbol{x}}} f_i(\boldsymbol{x}) \\ &amp; f_i(x) = 0, \ \ i = 1,\ldots, m \end{aligned} \right\} \Longrightarrow \nabla_{\boldsymbol{x}, \boldsymbol{\lambda}} \big[f_0(x) + \sum_{i=1}^p \lambda_i f_i(x)\big] = 0, \ \ \ \ 当\lambda_i \neq 0时, \ \ i = 1,\ldots, m \] 3.3.3. 一个不等式等式约束 假设目标函数\(\min f_0(x)\)， 有不等式约束\(f_1(x) &lt; 0\) 如图1表示， 当不等式约束有效时，须使得目标函数的函数值减小的方向与不等式约束成立的方向相反且目标函数与不等式约束的边界相切时取得极值. 即如果约束有效的情况下， 目标函数取得极值时一定在约束的边界处， 问题可以简化为等式约束的情况 边界有效时: 目标函数的函数值减小的方向与不等式约束成立的方向相反 则: \[\left. \begin{aligned} f_0(x)降低方向的梯度为: -\nabla f_0(x) &amp; \\ f_1(x) &lt; 0所表示定义域方向梯度方向为: -\nabla f_1(x) &amp; \\ f_0(x)与f_1(x)相切，且降低方向的梯度和定义域方向梯度相反 &amp; \end{aligned} \right\} \Longrightarrow \left\{ \begin{aligned} &amp; \nabla f_0(x) = -\lambda \nabla f_1(x) \\ &amp; \lambda &gt; 0 \end{aligned} \right. \] 考虑不等式约束无效的情况， 目标函数降低方向的梯度和定义域方向梯度相同， 即假设不等式约束成立的方向为图1中红色箭头相反的方向， 此时仍然可以有目标函数与不等式约束边界相切， 但是所求\(\lambda &lt; 0\)， 且此时切点对应的函数值显然不是极值处 另外，只考虑不等式约束时， 不等式自身约束恒成立力时(或着当有多个约束时， 其他约束的定义域为该约束的子集. 即该约束对所有约束的交集无贡献)， 该约束项的乘子可为0. 综上: 考虑约束的有效性综， 不等式约束的朗格朗日乘\(\lambda \leqslant 0\)， 等号拉格朗日乘子等于0时，代表约束对取得最值时无贡献 3.3.4. 多个不等式约束 多个不等式约束同多个不等式约束的情况. 其中每个不等式约束的朗格朗日乘子均大于等于0 3.3.5. 多个等式约束和多个不等式约束的情况 同多个等式约束情况， 取得极值处可看作是在可行域内有效约束内且由权值为拉格朗日乘子加权作为梯度平行于原函数的梯度 3.4. 原问题的拉格朗日函数 \[L_P= \min_{\boldsymbol{x}}\max_{\boldsymbol{\lambda}, \boldsymbol{\nu}} {L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\nu})}\] 3.5. 原问题的拉格朗日对偶函数 对偶函数 \[ 令, g(\boldsymbol{\lambda}, \boldsymbol{\nu}) = \inf_{x \in D} L(\boldsymbol{\lambda}, \boldsymbol{\nu}) = \inf_{x \in D}(f_0(x) + \sum_{i=1}^m\lambda_i f_i(x) + \sum_{i=1}^p \nu_i h_i(x)) \] \[L_D = \max_{\boldsymbol{\lambda}, \boldsymbol{\nu}}\min_{x} {L(x, \lambda, \nu)} = \max_{\boldsymbol{\lambda}, \boldsymbol{\nu}}g(\boldsymbol{\lambda}, \boldsymbol{\nu}) \] \(g(\boldsymbol{\lambda}, \boldsymbol{\nu})\)被求极小的部分： 可以看成给定一个\(\boldsymbol{x}\), 有\(常数 + 常数向量1 \cdot \boldsymbol{\lambda} + 常数向量2 \cdot \boldsymbol{\nu}\)， 即为关于\({\lambda}, \boldsymbol{\nu})\)仿射的； 考虑极小问题，是由无数个, 确定的\(\boldsymbol{x}\)对应的仿射函数逐点求下界，由仿射函数的下界求交集是凹的. 所以\(g(\boldsymbol{\lambda}, \boldsymbol{\nu})\) 是凹的 3.6. 原问题拉格朗日函数与对偶函数的关系 假设原问题和其对偶问题均有最优值 \[\begin{aligned} &amp; \min_{x} {L(x, \lambda, \nu)} \leqslant L(x, \lambda, \nu) \leqslant \max_{\lambda, \nu} {L(x, \lambda, \nu)} \\ \Longrightarrow \ &amp; \max_{\lambda, \nu}\min_{x} {L(x, \lambda, \nu)} \leqslant L(x, \lambda, \nu) \leqslant \min_{x}\max_{\lambda, \nu} {L(x, \lambda, \nu)} \\ \Longrightarrow \ &amp; L_D \leqslant L_P \end{aligned} \] 当上式子取得等号时， 称为强对偶. 3.7. Slater 准则 用于凸优化问题中，强对偶条件成立是否存在 \[\left. \begin{aligned} 原问题为凸优化问题 &amp; \\ 存在 x \in 约束条件的交集， 使得 f_i(x) &lt; 0, i = 1,\ldots, m &amp; \\ \end{aligned} \right\} \Longrightarrow 该问题的强对偶性可以达到 \] 对于在不等式约束函数为仿射函数的情况，只需要找到的\(x\)，满足原不等式即可（满足“\(\leqslant\)”， 而不需要更强的条件“\(&lt;\)”） 整理得： \[\left. \begin{aligned} 原问题为凸优化问题 &amp; \\ 存在 x \in 约束条件的交集且f_i(x)不是仿射的满足f_i(x) &lt; 0, \ i = 1,\ldots, k &amp; \\ \end{aligned} \right\} \Longrightarrow 该问题的强对偶性可以达到 \] 证明见:《凸优化》－ 清华出版社 Stephen Boyd 等著， 王书宁等译，\(P_{226} - P_{228}\) 3.8. KKT条件 强对偶成立时，最优解需要满足的条件 令\(x^*\)是原问题的最优解，\((\lambda^*， \nu^*)\)对偶问题的最优解 则， \[ \begin{eqnarray} 原始约束问题的最值=拉格朗日对偶问题的最值 \Rightarrow &amp; \ f_0(x^*) &amp; = &amp; g(\lambda^*, \nu^*) \\ 对偶问题的定义\Rightarrow &amp; &amp; = &amp; \inf_{x}\big(f_0(x) + \sum_{i=1}^m \lambda_i^* f_i(x) + \sum_{i=1}^p \nu^*h_i(x)\big) \\ 任意x的逐点求下解值小于其中一个x的值\Rightarrow &amp; &amp; \leqslant &amp; f_0(x^*) + \sum_{i=1}^m \lambda_i^* f_i(x^*) + \sum_{i=1}^p \nu^*h_i(x^*) \\ 不等式约束项小于等于0，等式约束项等于0 \Rightarrow &amp; &amp; \leqslant &amp; f_0(x^*) \\ \end{eqnarray} \] 由\(A \leqslant B \leqslant A\)形式得， \(B = A\) 即有： \[ \left. \begin{aligned} \left. \begin{aligned} \left. \begin{aligned} f_0(x^*) + \sum_{i=1}^m \lambda_i^* f_i(x^*) + \sum_{i=1}^p \nu_i^*h_i(x^*) = f_0(x^*) \\ 优化问题不等式约束，f_i(x) \leqslant 0 \\ 优化问题的等式约束， h_i(x) = 0 \\ \lambda_i \geqslant 0 \end{aligned} \right\} \Longrightarrow \lambda_i^*f_i(x^*) = 0， i = 1,2, \ldots, m \ \ \ \ &amp; \\ f_i(x) \leqslant 0， i = 1,2, \ldots, p \ \ \ \ &amp; \\ h_i(x) = 0， i = 1,2, \ldots, p \ \ \ \ &amp; \\ \lambda_i \geqslant 0， i = 1,2, \ldots, m \ \ \ \ &amp; \\ 拉格朗日函数在x^*处取得极小值： \nabla_{\boldsymbol{x}} \big[f_0(x) + \sum_{i=1}^m \lambda_i^* f_i(x) + \sum_{i=1}^m \nu_i^*h_i(x) \big]_{x=x^*} = 0 \ \ \ \ &amp; \end{aligned} \right\} KKT条件 \end{aligned} \right. \] 其中 \(\lambda_i^*f_i(x^*)\)为松弛条件 有： \[\left\{ \begin{aligned} &amp; 当 f_i(x) &lt; 0时， \lambda_i = 0. \ 该约束条件为非边界条件，不影响极值 \\ &amp; 当 f_i(x) = 0时， \lambda_i &gt; 0. \ 该约束条件为边界条件，在SVM中该点为支撑向量 \end{aligned} \right. \] 对于非凸问题， 拉格朗日函数的极值未必是原问题的最值， 所以未必是最优解。 对于凸的问题， 满足KKT条件即为原始约束问题的最优解， KKT条件是最优性充要条件 4. … 待续… 5. 参考资料 [1] 《凸优化》，清华出版社 Stephen Boyd，Lieven Vandenberghe著， 王书宁等译 [2] 维基百科-Lagrange multiplier https://en.wikipedia.org/wiki/Lagrange_multiplier [3] 维基百科-拉格朗日乘数 https://zh.wikipedia.org/wiki/拉格朗日乘数 [4]《统计学习方法》，李航著]]></content>
      <categories>
        <category>机器学习</category>
        <category>参数优化</category>
      </categories>
      <tags>
        <tag>凸优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python2.7编码处理小结]]></title>
    <url>%2F2016%2F04%2F23%2Fpython-encoding%2F</url>
    <content type="text"><![CDATA[Python2.7编程中遇到的一些编码问题小结 0.1. Python code中指定编码方式 1.指定源文件的的编码方式 作用:解决python解释器载入py程序文件时编码报错问题 1#coding=utf8 1#coding:utf8 1# -*- coding: UTF-8 -*- 以上方式效果是等价的,选择一种使用即可 2.指定中文代码默认解码方式 作用:解决解释器环境输出文本时乱码的部分问题 123import sysreload(sys)sys.setdefaultencoding('utf-8') 关于reload(sys)?: setdefaultencoding函数在第一次系统调用后会被删除,另外由于python的import的“ifndefine”机制,import sys不能保证sys被加载,所以由reload显式加载sys 0.2. 关于未知编码转utf8编码 一个实际数据场景:数据来源和存储形式多样,编码多样,需要将不同编码转化为utf8编码进行后续加工处理 123456import chardetbuf = '这是一个编码'str_detect_dict = chardet.detect(buf)buf = unicode(buf, str_detect_dict['encoding'], "ignore")print buf, str_detect_dict&gt;&gt;这是一个编码 &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125; 以上代码可以解决遇到的比较多得编码转换问题,但之前有碰到一些调皮些的编码问题 123456import chardetbuf = '\u8fd9\u662f\u4e00\u4e2a\u7f16\u7801\u95ee\u9898'str_detect_dict = chardet.detect(buf)buf = unicode(buf, str_detect_dict['encoding'], "ignore")print buf, str_detect_dict&gt;&gt;\u8fd9\u662f\u4e00\u4e2a\u7f16\u7801\u95ee\u9898 &#123;'confidence': 1.0, 'encoding': 'ascii'&#125; 好尴尬,chardet.detect以绝对的自信检测出该编码为ascii,所以unicode函数没有起作用 0.2.1. 介绍两个函数 repr(): 返回机器存储方式 1234567buf = u'这是一个编码'print repr(buf)&gt;&gt;u'\u8fd9\u662f\u4e00\u4e2a\u7f16\u7801'buf = '这是一个编码'print repr(buf) &gt;&gt;'\xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbc\x96\xe7\xa0\x81' eval(): 将字符串str当成有效的表达式来运行并返回结果 123result = eval('12+3')print type(result), result&gt;&gt;&lt;type 'int'&gt; 15 是不是想到了sql注入攻击? 0.2.2. 回到调皮编码的问题 1buf = '\u8fd9\u662f\u4e00\u4e2a\u7f16\u7801\u95ee\u9898' 或 buf = '乱码字符串' 以上编码离repr(‘这是一个编码’)在存储上只差一个’u’ 个人处理chardet.dect解决后仍有问题的编码,通常按一下方式解决: &gt;1.透过现象看本质,repr()后查看字符串的机器存储方式 &gt;2.找规律,找这些编码方式和已知编码方式的相似处和区别 &gt;3.手术刀,修改机器存储方式,通过eval()函数整合为新的编码的字符串 解决问题过程大概如下: 123456789101112import chardetbuf = '\u8fd9\u662f\u4e00\u4e2a\u7f16\u7801\u95ee\u9898' #或 buf = '一堆乱码'print repr(buf)&gt;&gt;'\\u8fd9\\u662f\\u4e00\\u4e2a\\u7f16\\u7801\\u95ee\\u9898'new_buf_repr = 'u"%s"' % buf #或new_buf_repr = 'u"%s"' % repr(buf)print new_buf_repr&gt;&gt;"\u8fd9\u662f\u4e00\u4e2a\u7f16\u7801\u95ee\u9898"new_buf = eval(new_buf_repr)print type(new_buf), new_buf&gt;&gt;&lt;type 'unicode'&gt; 这是一个编码问题 0.2.3. 这种数据虽然有点狗血.如果是成批量的话可参考以上解决方式.另外极少数的编码方式是混合编码,这部分有要的时候需要用re来解决]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>