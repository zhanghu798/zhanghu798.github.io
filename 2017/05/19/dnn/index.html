<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="DNN,BP,xavier,BN," />








  <link rel="shortcut icon" type="image/x-icon" href="/assets/blogImg/zhanghu798.jpg?v=5.1.0" />






<meta name="description" content="Deep Neural Networks
1. 人工神经网络介绍
https://zh.wikipedia.org/wiki/人工神经网络
2. DNN与传统机器学习方法对比

优点

降低或避免对特征工程的依赖，深度学习能做特征工程就是因为深度或广度，再加上非线性激励使得更多的组合特征出现，用损失函数来约束整个网络的发展方向，结果就是选择出来的比较容易过拟合的特征。但是这些特征没有什么可解释性，">
<meta property="og:type" content="article">
<meta property="og:title" content="深度神经网络简介">
<meta property="og:url" content="http://yoursite.com/2017/05/19/dnn/index.html">
<meta property="og:site_name" content="张虎的博客">
<meta property="og:description" content="Deep Neural Networks
1. 人工神经网络介绍
https://zh.wikipedia.org/wiki/人工神经网络
2. DNN与传统机器学习方法对比

优点

降低或避免对特征工程的依赖，深度学习能做特征工程就是因为深度或广度，再加上非线性激励使得更多的组合特征出现，用损失函数来约束整个网络的发展方向，结果就是选择出来的比较容易过拟合的特征。但是这些特征没有什么可解释性，">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/dnn_struct.png">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/dnn_neurons.png">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/sigmoid.png">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/tanh.png">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/relu.png">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/leaky_relu.png">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/dnn_bp_example.png">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/bn_transform.png">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/bn_gradient.png">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/dnn_dropout.png">
<meta property="og:image" content="http://yoursite.com/pic/ml/dnn/dropout_standard_vs_drop.png">
<meta property="og:updated_time" content="2017-05-23T04:20:58.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度神经网络简介">
<meta name="twitter:description" content="Deep Neural Networks
1. 人工神经网络介绍
https://zh.wikipedia.org/wiki/人工神经网络
2. DNN与传统机器学习方法对比

优点

降低或避免对特征工程的依赖，深度学习能做特征工程就是因为深度或广度，再加上非线性激励使得更多的组合特征出现，用损失函数来约束整个网络的发展方向，结果就是选择出来的比较容易过拟合的特征。但是这些特征没有什么可解释性，">
<meta name="twitter:image" content="http://yoursite.com/pic/ml/dnn/dnn_struct.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 'undefined',
      author: '博主'
    },
    algolia: {
      applicationID: '49YSP050FE',
      apiKey: 'dcca29b3b14f21decd6cab3544e195fe',
      indexName: 'reset.pub',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/05/19/dnn/"/>





  <title> 深度神经网络简介 | 张虎的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?857a9d776bb324f3f63614da1e8c26d0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">张虎的博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <h1 class="site-subtitle" itemprop="description"></h1>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input" placeholder="search my blog...">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/19/dnn/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张虎">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/assets/blogImg/zhanghu798.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="张虎的博客">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="张虎的博客" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
            
            
              
                深度神经网络简介
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-19T23:28:23+08:00">
                2017-05-19
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2017-05-23T12:20:58+08:00">
                2017-05-23
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/DL/" itemprop="url" rel="index">
                    <span itemprop="name">DL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/05/19/dnn/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/05/19/dnn/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/05/19/dnn/" class="leancloud_visitors" data-flag-title="深度神经网络简介">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote class="blockquote-center"><font size="4">Deep Neural Networks</font></blockquote>
<h1 id="人工神经网络介绍">1. 人工神经网络介绍</h1>
<p><a href="https://zh.wikipedia.org/wiki/人工神经网络" class="uri" target="_blank" rel="external">https://zh.wikipedia.org/wiki/人工神经网络</a></p>
<h1 id="dnn与传统机器学习方法对比">2. DNN与传统机器学习方法对比</h1>
<ul>
<li>优点
<ul>
<li>降低或避免对特征工程的依赖，深度学习能做特征工程就是因为深度或广度，再加上非线性激励使得更多的组合特征出现，用损失函数来约束整个网络的发展方向，结果就是选择出来的比较容易过拟合的特征。但是这些特征没有什么可解释性，跟多是黑盒</li>
</ul></li>
<li>缺点
<ul>
<li>黑盒模型</li>
<li>需要大样本支撑<br>
</li>
<li>对硬件要求高</li>
</ul></li>
</ul>
<a id="more"></a>
<h1 id="dnn基本结构">3. DNN基本结构</h1>
<p>输入层，隐藏层，隐藏层，… ， 输出层</p>
<img src="/pic/ml/dnn/dnn_struct.png" border="0" width="70%" height="70%" style="margin: 0 auto">
<center>
<a href="https://ljalphabeta.gitbooks.io/neural-networks-and-deep-learning-notes/content/chapter5.html" target="_blank" rel="external">图1，神经网络结构示意图</a>
</center>
<img src="/pic/ml/dnn/dnn_neurons.png" border="0" width="50%" height="50%" style="margin: 0 auto">
<center>
<a href="https://zh.wikipedia.org/wiki/人工神经网络" target="_blank" rel="external">图2，神经元（感知器）示意图</a>
</center>
<h1 id="激励函数">4. 激励函数</h1>
<p>激励函数的作用，增加非线形特征映射。激励函数的对比见：<a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html" target="_blank" rel="external">Must Know Tips/Tricks in Deep Neural Networks</a></p>
<h2 id="sigmoid函数">4.1. Sigmoid函数</h2>
<p><span class="math display">\[f(x) = \frac{1}{1+e^{-x}}\]</span></p>
<img src="/pic/ml/dnn/sigmoid.png" border="0" width="40%" height="40%" style="margin: 0 auto">
<center>
<a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html" target="_blank" rel="external">图3，sigmoid(x)</a>
</center>
<h2 id="tanh函数">4.2. tanh函数</h2>
<p><span class="math display">\[
tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]</span></p>
<img src="/pic/ml/dnn/tanh.png" border="0" width="40%" height="40%" style="margin: 0 auto">
<center>
<a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html" target="_blank" rel="external">图4，tanh(x)</a>
</center>
<h2 id="relu">4.3. Relu</h2>
<p><span class="math display">\[f(x) = \max(0, x)\]</span></p>
<img src="/pic/ml/dnn/relu.png" border="0" width="40%" height="40%" style="margin: 0 auto">
<center>
<a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html" target="_blank" rel="external">图5，relu(x)</a>
</center>
<h2 id="leaky-relu">4.4. Leaky ReLU</h2>
<p><span class="math display">\[
f(x) = \left\{
\begin{aligned}
x，&amp; x \geqslant 0  \\
\alpha x， &amp;  x &lt; 0 \\
\end{aligned}
\right.
\]</span></p>
<img src="/pic/ml/dnn/leaky_relu.png" border="0" width="40%" height="40%" style="margin: 0 auto">
<center>
<a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html" target="_blank" rel="external">图6，leaky-relu(x)</a>
</center>
<p>更多激励函数见<a href="https://en.wikipedia.org/wiki/Activation_function" target="_blank" rel="external">wiki</a></p>
<h1 id="神经网络训练方法">5. 神经网络训练方法</h1>
<p>小批量梯度下降（Mini-batch Stochastic Gradient Descent）</p>
<p>小批量随机梯度下降较全量样本的梯度下降特点：<br>
- 收敛速度快，收随机梯度下降只是用少量的样本即可完成一次迭代<br>
- 可以跳出局部最优解，找到更接近的全局最优的解，这大概是DNN随机梯度下降而不是全量梯度下降的最直接原因</p>
<h2 id="参数训练流程">5.1. 参数训练流程</h2>
<p>1，定义损失函数，初始化参数：<br>
2，正向传播得到损失<br>
3，反向传播更新参数<br>
4，判断是停止，否：则跳转到第2步</p>
<h2 id="反向传播算法">5.2. 反向传播算法</h2>
<p>Backpropagation，BP</p>
<p>由最后一层逐层向前更新参数的算法</p>
<h3 id="剃度计算及bp推导">5.2.1. 剃度计算及BP推导</h3>
<p>以全链接方式（图一）为结果，图二的方式的神经元链接方式为例：</p>
<ul>
<li>符号说明：
<ul>
<li><span class="math inline">\(W_{i, j, k}\)</span>：第<span class="math inline">\(i\)</span>层神经元的第<span class="math inline">\(j\)</span>个神经元与第<span class="math inline">\(i+1\)</span>层的第<span class="math inline">\(k\)</span>个神经元的链接权重</li>
<li><p><span class="math inline">\(O_{i, j}\)</span>：第<span class="math inline">\(i\)</span>层神经元的第<span class="math inline">\(j\)</span>个神经元的<strong>数学表达式</strong></p>
<p><span class="math display">\[
O_{i\ \ ,\ \ j_{i}} = 
\left\{
\begin{aligned}
&amp; f\Bigg(\sum_{j_{i-1}\ \ =1}^{J_{i-1}} W_{i-1\ \ , \ \ j_{i-1}\ \ ,\ \  j_{i}} \cdot O_{i-1\ \ , \ \ j_{i-1}} + b_{i\ \ ,\ \ j_{i}} \Bigg) \ \ \ &amp; &amp;  当O_{i\ ,\ \  j_{i}}为神经元  \\
&amp; x_{i\ ,\ j_{i}} &amp; &amp; 当O_{i\ ,\  j_{i}}为原始输入时 
\end{aligned}
\right. \tag{1}
\]</span></p></li>
<li><p><span class="math inline">\(J_{i}\)</span>： 第<span class="math inline">\(i\)</span>层神经元个数</p></li>
<li><p><span class="math inline">\(IN_{i,\ j_i}\)</span>： 第<span class="math inline">\(i\)</span>层的第<span class="math inline">\(j_i\)</span>个神经元激励函数输入值的<strong>数学表达式</strong>，图2中激励函数f输入部分的数学表达式 <span class="math display">\[
IN_{i,\ j_i} = \sum_{j_{i-1}\ \ =1}^{J_{i-1}} W_{i-1\ \ , \ \ j_{i-1}\ \ ,\ \  j_{i}} \cdot O_{i-1\ ,\ j-1} + b_{i\ ,\ j_{i}} 
\tag{2}
\]</span></p></li>
<li><span class="math inline">\(V_{IN_{i,\ j_i}}\)</span>：第<span class="math inline">\(i\)</span>层的第<span class="math inline">\(j_i\)</span>个神经元的<span class="math inline">\(IN_{i,\ j_i}\)</span>的值，图2激励函数的输入部分<strong>数值</strong></li>
<li><p><span class="math inline">\(f^\prime \big(V_{IN_{i,\ j_i}} \ \big)\)</span>：第<span class="math inline">\(i\)</span>层的第<span class="math inline">\(j_i\)</span>个神经元的激励函数（或损失函数）导数在<span class="math inline">\(V_{IN_{i,\ j_i}}\)</span>处的<strong>数值</strong></p></li>
</ul></li>
</ul>
<p>考虑第<span class="math inline">\(i\)</span>层的第<span class="math inline">\(j\)</span>个神经元与<span class="math inline">\(i+1\)</span>层的第<span class="math inline">\(k\)</span>神经元之间的链接权重<span class="math inline">\(W_{i, j, k}\)</span>的，气候其后面链接神经元的计算表达式中含有<span class="math inline">\(W_{i, j, k}\)</span>项的是：第<span class="math inline">\(i+1\)</span>层第<span class="math inline">\(k\)</span>个神经元，第<span class="math inline">\(i+2\)</span>层到到<span class="math inline">\(n\)</span>层的所有神经元</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\frac{\partial{O_{n,\ j_{n}} }}{\partial W_{i,\ j,\ k}} \\
= &amp; \left.\frac
    {\partial f(x)}
    {\partial x } 
   \right|_{x=V_{IN_{n\ ,\  j_{n}}}}
   \ \cdot \ 
   \frac
    {\partial \Big( b_{n, x}  + \ \sum_{j_{n-1}\ =1}^{J_{n-1}} O_{n-1, \ j_{n-1}} \   \cdot \  W_{n-1,\ j_{n-1}\ \ ,\ j_{n}} \ \Big) } 
    {\partial W_{i,\ j,\ k}} \\
= &amp; f^\prime \big(V_{IN_{n,\ j_{n}}} \ \big) \ \cdot \ \sum_{j_{n-1}\ \ =1}^{J_{n-1}} W_{n-1,\ j_{n-1}\ \ , \ j_{n}} \ \cdot \ \frac{\partial{O_{n-1,\ j_{n-1}} }}{\partial W_{i,\  j,\ k}} \\
\end{aligned} \tag{3}
\]</span></p>
<p>直接推导（剃度的通项公式）： <span class="math display">\[
\begin{aligned}
\frac{\partial{O_{n\ ,\ j_{n}} }}{\partial W_{i\ ,\ j\ ,\ k}} 
= &amp; f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \big) \ 
    \cdot \  
    \sum_{j_{n-1}\ \ =1}^{J_{n-1}} W_{n-1\ ,\ j_{n-1}\ , \ x} \ 
    \cdot \ 
    \frac{\partial{O_{n-1\ ,\ j_{n-1}} }}{\partial W_{i\ ,\ j\ ,\ k}} \\
= &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}} f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \big)  \ 
    \cdot \  
    W_{n-1\ ,\ j_{n-1}\ \ , \ j_{n}} \ 
    \cdot \ 
    \frac{\partial{O_{n-1\ ,\ j_{n-1}} }}{\partial W_{i\ ,\ j\ \ ,\ k}} \\
= &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}} f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \ \ \big)  \  
    \cdot \  
    W_{n-1\ ,\ j_{n-1}\ \ , \ j_{n}} \ 
    \cdot \  
    \Bigg( \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} f^\prime \big(V_{IN_{n-1,\ j_{n-1}}} \ \ \ \big)  \ 
    \cdot \     
    W_{n-2\ ,\ j_{n-2}\ \ , \ j_{n-1}} \ 
    \cdot \ 
    \frac{\partial{O_{n-2\ ,\ j_{n-2}} }}{\partial W_{i\ ,\ j\ \ ,\ k}} \Bigg) \\
= &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}} \ \ \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}}  \ 
    \cdot\ 
    f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \ \ \big)  \ 
    \cdot \    
    W_{n-1\ ,\ j_{n-1}\ \ , \ j_{n}} \ 
    \cdot \ 
    f^\prime \big(V_{IN_{n-1,\ j_{n-1}}} \ \ \ \big)  \ 
    \cdot \     
    W_{n-2\ ,\ j_{n-2}\ \ , \ j_{n-1}} \ 
    \cdot \ 
    \frac{\partial{O_{n-2\ ,\ j_{n-2}} }}{\partial W_{i\ ,\ j\ \ ,\ k}} \\
= &amp; \dots \\
= &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}}\ \  \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} \dots   \sum_{j_{i+2}\ \ = \ 1}^{J_{i+2}}  \ 
    \cdot\ 
     f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \ \ \big) \\
  &amp; \cdot 
      \Big(W_{n-1\ ,\ j_{n-1} \ \ , \ j_{n}}  \ 
     \cdot\
      f^\prime \big(V_{IN_{n-1\ ,\ j_{n-1}}} \ \ \ \big) \Big) \\
  &amp; \cdot 
      \Big(W_{n-2\ ,\ j_{n-2} \ \ , \ j_{n-1}}  \ 
     \cdot\
      f^\prime \big(V_{IN_{n-2\ ,\ j_{n-2}}} \ \ \ \big) \Big) \\
  &amp;\cdot \dots \cdot\ 
      \Big(W_{x\ ,\ j_{x} \ \ , \ j_{x+1}} 
        \cdot  
        f^\prime \big(V_{IN_{x\ ,\ j_{x}}} \ \ \ \big)  \Big) \cdot \dots \cdot\ \\
  &amp;  \cdot\
  \Big( W_{i+2\ ,\ j_{i+2} \ \ , \ j_{i+3}}
    \cdot  
    f^\prime \big(V_{IN_{i+2\ ,\ j_{i+2}}} \ \ \ \big)  \Big) \\
  &amp; \cdot \
        \Big(
        W_{i+1\ ,\ k \ \ , \ j_{i+2}}
        \cdot \ 
        \frac{\partial{O_{i+1\ ,\ k} }}{\partial W_{i\ ,\ j\ ,\ k}}  
  \Big) \\
= &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}}\ \  \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} \dots   \sum_{j_{i+2}\ \ = \ 1}^{J_{i+2}}  \
    \cdot\ 
     f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \ \ \big) \\
  &amp; \cdot 
    \prod_{x={i+2}}^{n-1}\Big(W_{x\ ,\ j_{x} \ \ , \ j_{x+1}} 
        \cdot  
        f^\prime \big(V_{IN_{x\ ,\ j_{x}}} \ \ \ \big)  \Big)  \\
  &amp; \cdot \
    \Big(
        W_{i+1\ ,\ k \ \ , \ j_{i+2}}
        \cdot \ 
        \frac{\partial{O_{i+1\ ,\ k} }}{\partial W_{i\ ,\ j\ ,\ k}}  
    \Big) 
\end{aligned} \tag{4}
\]</span></p>
<p>BP算法推导（剃度的由后往前的递推公式）：</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial{O_{n\ ,\ j_{n}} }}{\partial O_{i+1\ ,\ j_{i+1}}} 
= &amp; \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}}\ \  \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} \dots   \sum_{j_{i+2}\ \ = \ 1}^{J_{i+2}}  \
    \cdot\ 
     f^\prime \big(V_{IN_{n\ ,\ j_{n}}} \ \ \ \big) \\
  &amp; \cdot 
    \prod_{x={i+2}}^{n-1}\Big(W_{x\ ,\ j_{x} \ \ , \ j_{x+1}} 
        \cdot  
        f^\prime \big(V_{IN_{x\ ,\ j_{x}}} \ \ \ \big)  \Big)  \\
  &amp; \cdot \
    \Big(
        W_{i+1\ ,\ j_{i+1} \ \ , \ j_{i+2}} \cdot \ 1 
    \Big) 
\end{aligned} \tag{5}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial{O_{n\ ,\ j_{n}} }}{\partial O_{i\ ,\ j_{i}}} 
= &amp; \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ \cdot \  \sum_{j_{n-1}\ \ = \ 1}^{J_{n-1}} \sum_{j_{n-2}\ \ = \ 1}^{J_{n-2}} \dots   \sum_{j_{i+2}\ \ = \ 1}^{J_{i+2}}  \ 
    \cdot\ 
     f^\prime \big(V_{IN_{n\ ,\ k}} \ \big) \\
  &amp; \cdot 
    \prod_{x={i+2}}^{n-1}\Big(W_{x\ ,\ j_{x} \ \ , \ j_{x+1}} 
        \cdot  
        f^\prime \big(V_{IN_{x\ ,\ j_{x}}} \ \ \ \big)  \Big)  \\
  &amp; \cdot \Big(W_{i+1\ ,\ j_{i+1} \ \ , \ j_{x+2}} 
        \cdot  
        f^\prime \big(V_{IN_{i+1\ ,\ j_{i+1}}} \ \ \ \big)  \Big)  \\
  &amp; \cdot \
    \Big(
        W_{i\ ,\ j_{i} \ \ , \ j_{i+1}} 
    \Big)
\end{aligned} \tag{6}
\]</span></p>
<p>对比式（5），式（6）得： <span class="math display">\[
\begin{equation}
\boxed{
    \frac{\partial{O_{n\ ,\ k} }}{\partial O_{i\ ,\ j_{i}}} 
    = \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ 
        f^\prime \big(V_{IN_{i+1\ ,\ j_{i+1}}} \ \ \ \ \big) \ 
        \cdot \ 
        W_{i\ ,\ j_{i} \ \ , \ j_{i+1}} \ 
        \cdot \
        \frac{\partial{O_{n\ ,\ k} }}{\partial O_{i+1\ ,\ j_{i+1}}}
}
\ \ \ \ （其中i + 1 \leqslant n）
\end{equation} \tag{7}
\]</span></p>
<h3 id="使用bp-mini-batch-sgd训练的流程">5.2.2. 使用BP + Mini-batch SGD训练的流程</h3>
<p>假设损失函数为<span class="math inline">\(\ell = (y_i - \hat{y})^2\)</span>， 第n层输出层，则式（7）中的第n层第k个神经元可以看作是计算损失的单元，即： <span class="math display">\[
O_{n, k}=(y_i - \hat{y})^2
\tag{8}
\]</span></p>
<p><span class="math display">\[
V_{IN_{\ell}} = V(O_{n, k})
\tag{9}
\]</span></p>
<p><span class="math display">\[
\ell^\prime \big(V_{IN_{\ell}} \ \big) = 2V_{IN_{\ell}} 
\tag{10}
\]</span></p>
<p>针对最后一层隐层到输出层的梯度为： <span class="math display">\[
\frac{\partial{\ell}}{\partial W_{n\ ,\ j_{n}}} 
= \ell^\prime \big(V_{IN_{\ell}} \ \big) \cdot \ V_{O_{n\ ,\ j_{n} }}
\ \ \ \ j_{n} = 1, 2, \dots , J_{n}
\tag{11}
\]</span></p>
<p><span class="math display">\[
\frac{\partial{\ell}}{\partial O_{n\ ,\ j_{n}}} 
= \ell^\prime \big(V_{IN_{\ell}} \ \big) \cdot \ W_{n\ ,\ j_{n} }
\ \ \ \ j_{n} = 1, 2, \dots , J_{n}
\tag{12}
\]</span></p>
<p>则，针对任意层的神经元反向传播梯度为： <span class="math display">\[
\begin{aligned}
&amp; \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} 
    = \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ 
        f^\prime \big(V_{IN_{i+1\ ,\ j_{i+1}}} \ \ \  \big) \ 
        \cdot \ 
        W_{i\ ,\ j_{i} \ \ , \ j_{i+1}} \ 
        \cdot \
        \frac{\partial{\ell}}{\partial O_{i+1\ ,\ j_{i+1}}}
\end{aligned}
\tag{13}
\]</span></p>
<p>任意层权值<span class="math inline">\(w\)</span>的反向传播梯度为： <span class="math display">\[
\begin{aligned}
\frac{\partial{\ell}}{\partial W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}} 
= &amp; \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} 
\ \cdot \ 
\frac{\partial O_{i\ ,\ j_{i}}} {\partial W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}} \\
= &amp; \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} 
\ \cdot \ 
f^\prime \big(V_{IN_{i\ ,\ j_{i}}} \ \big) 
\ \cdot \ 
\frac{\partial V_{IN_{i\ ,\ j_{i}}} } {\partial W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}} \\
= &amp; \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} 
\ \cdot \ 
f^\prime \big(V_{IN_{i\ ,\ j_{i}}} \ \big) 
\ \cdot \ 
V_{O_{i-1\ \ ,\ j_{i-1}}}
\end{aligned}
\tag{14}
\]</span></p>
<p>参数范围： <span class="math display">\[
\left\{
\begin{aligned}
&amp; i = 2, 3, \dots, n-1 \\
&amp; j_i = 1, 2, 3, \dots, J_i \\
&amp; j_{i+1} = 1, 2, 3, \dots, J_{i+1}
\end{aligned}
\right.
\tag{15}
\]</span></p>
<p>将偏移项对应的输入值看看成1，即<span class="math inline">\(V_{O_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}} = 1\)</span>，则上式中<span class="math inline">\(W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}\)</span>可看成是常数1的权重，并且这个权重影响到的是<span class="math inline">\(O_{i,\ \ j{i}}\)</span>, 那么将此权重写作<span class="math inline">\(b_{i,\ \ j{i}}\)</span>表示第<span class="math inline">\(i\)</span>层<span class="math inline">\(j_{i}\)</span>个神经元求和项的偏置 <span class="math display">\[
\frac{\partial{\ell}}{\partial b_{i\ \ ,\ j_{i}}}  = \frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}} 
\ \cdot \ 
f^\prime \big(V_{IN_{i\ ,\ j_{i}}} \ \big)
\tag{16}
\]</span></p>
<p>对于学习率为<span class="math inline">\(\alpha\)</span>，batch为<span class="math inline">\(M\)</span>的SGD权值逐层更新公式如下： <span class="math display">\[
\boxed{
W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}} = W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}} 
-
\alpha \cdot \sum_{m=1}^{M} \frac{\partial{\ell}}{\partial W_{i-1\ \ ,\ j_{i-1}\ \ , \ \ j_{i}}} 
}
\tag{17}
\]</span></p>
<h3 id="反向传播简单例子">5.2.3. 反向传播简单例子</h3>
<span class="math display">\[f(\boldsymbol{x}) = \frac{1}{1 + \exp^{-(w_{0} x_{0} + w_{1} x_{1} + w_{2})}}的BP示意图 \]</span> <img src="/pic/ml/dnn/dnn_bp_example.png" border="0" width="80%" height="80%" style="margin: 0 auto">
<center>
<a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="external">图2，BP示意图</a>
</center>
<h2 id="xavier参数初始化">5.3. Xavier参数初始化</h2>
<p>2010 Xavier Glorot， Yoshua Bengio Understanding the difficulty of training deep feedforward neural networks, <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" class="uri" target="_blank" rel="external">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a><br>
<a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers/initializers" class="uri" target="_blank" rel="external">https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers/initializers</a><br>
<a href="http://blog.csdn.net/app_12062011/article/details/57956920" class="uri" target="_blank" rel="external">http://blog.csdn.net/app_12062011/article/details/57956920</a></p>
<h3 id="问什么要使用初始化">5.3.1. 问什么要使用初始化</h3>
<p>主要目的是使的训练更容易进行下去</p>
<p>考虑计算机的数值运算时的上溢及下溢存在，对应避免方差过大导致剃度爆炸及剃度消失</p>
<h3 id="基本假设">5.3.2. 基本假设</h3>
<p>输入样本x，权重w，偏置b分别服从均值为0的分布，所有样本属于独立同分布，所有权重属于独立同分布</p>
<p>关于原点对称的激活函数，即满足<span class="math inline">\(f^\prime(x) \approx 1\)</span>，满足该条件的激励函数有Softsig， tanh， relu，所以以下讨论只是针对这些激励函数，而不包括sigmoid函数。</p>
<p>因为激励函数的0导数为1，且在x，w，b均值为0的假设下，每个神经元激励的输入的均值0附近。为了方便推导，将激励函数近似成<em>线性函数</em> 即，对于激励函数的输入<span class="math inline">\(In\)</span>,激励函数的出处<span class="math inline">\(Out\)</span> <span class="math display">\[Out=f(In)=In \tag{18}\]</span></p>
<h3 id="方差公式">5.3.3. 方差公式</h3>
<p>用到的几个<a href="https://en.wikipedia.org/wiki/Variance#Product_of_independent_variables" target="_blank" rel="external">方差公式</a><br>
如果有均值为0的两个互相独立的变量<span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>。则<span class="math inline">\(Var(xy) = Var(x) Var(y)\)</span><br>
对于常数<span class="math inline">\(a\)</span>，有<span class="math inline">\(Var(x+a) = Var(x)\)</span></p>
<h3 id="目标函数">5.3.4. 目标函数</h3>
<h4 id="保证正向传播方差一致">5.3.4.1. 保证正向传播方差一致</h4>
<p>任意两层网络激励函数的输出方差相等.</p>
<p>目标函数 <span class="math display">\[
\forall(i, i^\prime)，\ \  Var[O^{i}] = Var[O^{i^\prime}] \tag{19}
\]</span></p>
<p>根据式（2）， <span class="math display">\[
IN_{i,\ j_i} = \sum_{j_{i-1}\ \ =1}^{J_{i-1}} W_{i-1\ \ , \ \ j_{i-1}\ \ ,\ \  j_{i}} \cdot O_{i-1\ \ , \ \ j_{i-1}} + b_{i\ ,\ j_{i}} 
\]</span></p>
<p>激励函数输入经过经过激励函数后得到激励函数的输出： <span class="math display">\[
O_{i,\ j_i} = f(IN_{i,\ j_i})
\]</span></p>
<p>利用式（19）的激励函数的近似得 <span class="math display">\[
O_{i,\ j_i} = \sum_{j_{i-1}\ \ =1}^{J_{i-1}} W_{i-1\ \ , \ \ j_{i-1}\ \ ,\ \  j_{i}} \cdot O_{i-1\ \ , \ \ j_{i-1}} + b_{i\ ,\ j_{i}} 
\]</span></p>
<p>计算方差 <span class="math display">\[
Var(O_{i,\ j_i}) = \sum_{j_{i-1}\ \ =1}^{J_{i-1}} Var(W_{i-1\ \ , \ \ j_{i-1}\ \ ,\ \  j_{i}}) \cdot Var(O_{i-1\ \ , \ \ j_{i-1}} )
\]</span></p>
<p>合并相同方差 <span class="math display">\[
Var(O_{i}) = J_{i-1} Var(W_{i-1}) \cdot Var(O_{i-1})
\]</span></p>
<p>可以将以上递推公式展开得到论文中的形式，但是使用递推公式既可得到结论</p>
<p>要使得目标函数成立，需使得每一层的神经元输出的方差都和下一层相同，即，需使得 <span class="math display">\[
J_{i} Var(W_{i}) = 1 ， \ \ i = 1, 2, \cdots, n
\tag{20}
\]</span></p>
<h4 id="保证反向传播的方差一致">5.3.4.2. 保证反向传播的方差一致</h4>
<p>任意两层网络的损失损失对激励层输入的导数相等</p>
<p>目标函数：<br>
<span class="math display">\[
\forall(i, i^\prime)，\ \   \frac{\partial{\ell}}{\partial IN_{i}} = \frac{\partial{\ell}}{\partial IN_{i^\prime}} \tag{21}
\]</span></p>
<p>在式（5），式（6），式（7）的基础上可以容易得到： <span class="math display">\[
\frac{\partial{\ell }}{\partial IN_{i\ ,\ j_{i}}} 
    = \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ 
            \cdot \
        W_{i\ ,\ j_{i} \ \ , \ j_{i+1}} \ 
        f^\prime \big(V_{IN_{i\ ,\ j_{i}}} \big) \ 
        \cdot \
        \frac{\partial{\ell}}{\partial IN_{i+1\ ,\ j_{i+1}}}
\]</span> 计算方差 <span class="math display">\[
Var(\frac{\partial{\ell }}{\partial IN_{i\ ,\ j_{i}}}) 
    = \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ 
            \cdot \
        Var(W_{i\ ,\ j_{i} \ \ , \ j_{i+1}}) \ 
        \cdot \
        Var(f^\prime \big(V_{IN_{i\ ,\ j_{i}}} \big)) \ 
        \cdot \
        Var(\frac{\partial{\ell}}{\partial IN_{i+1\ ,\ j_{i+1}}})
\]</span> 相等方差合并 <span class="math display">\[
Var(\frac{\partial{\ell }}{\partial IN_{i}}) 
    = J_{i+1}\ 
            \cdot \
        Var(W_{i}) \ 
        \cdot \
        Var(\frac{\partial{\ell}}{\partial IN_{i+1}})
\]</span></p>
<p>则，要使得式（20）成立，需使得 <span class="math display">\[
J_{i+1}Var(W_{i}) = 1， \ \ i = 1, 2, \cdots, n
\tag{22}
\]</span></p>
<h3 id="结论">5.3.5. 结论</h3>
<p>当<span class="math inline">\(J_{i} \neq J_{i+1}\)</span> 时式（20）和式（22）同时满足的话是矛盾的，但是论文中提出了一种折中方法，使用两种情况的<strong>调和平均数</strong> <span class="math display">\[
Var(W_{i}) = \frac{2}{J_{i} + J_{i+1}}
\]</span></p>
<p>均值为0的均匀分布可以表示为[-a, a]。令其方差为<span class="math inline">\(\frac{2}{J_{i} + J_{i+1}}\)</span>，则有 <span class="math display">\[
\frac{(a-(-a))^2}{12} =  \frac{2}{J_{i} + J_{i+1}}
\]</span> 得 <span class="math display">\[
a = \frac{\sqrt{6}}{\sqrt{J_{i} + J_{i+1}}}
\]</span></p>
<p>则 <span class="math display">\[
w \sim U\Bigg[-\frac{\sqrt{6}}{\sqrt{J_{i} + J_{i+1}}}, \frac{\sqrt{6}}{\sqrt{J_{i} + J_{i+1}}}\Bigg]
\tag{23}
\]</span></p>
<h3 id="特别说明">5.3.6. 特别说明</h3>
<ul>
<li>式（23）是针对激励函数<span class="math inline">\(f\)</span>，有<span class="math inline">\(f^\prime(x) \approx 1\)</span>，且有<span class="math inline">\(f(0)=0\)</span>条件的推导。基于以上两个条件近似f(x)=x。所以上面推导并不适用于sigmoid激励函数。<br>
</li>
<li>因为推导过程中激励函数使用了近似，另外正反向传播方差方差的不一致使用了折中的调和平均，所以针对特别深的网络Xavier初始化并不总是有用。<br>
</li>
<li>因为使用了激励函数的近似在0处的近似替代，则数据输入层尽可能在均值为0，可以去均值或归一化等。</li>
</ul>
<h2 id="batch-normalization">5.4. Batch Normalization</h2>
<p>主要参考：<a href="https://arxiv.org/abs/1502.03167" class="uri" target="_blank" rel="external">https://arxiv.org/abs/1502.03167</a></p>
<p>为了避免梯度消失导致深度神经网络无法训练的情况，通常使用relu激励函数+vavier的权值初始化方式</p>
<p>如果使用sigmoid的情况下，尽量使得各个神经元求和之后尽量为均值为0的高斯分布中，</p>
<p>将过激励函数的输入数据的每一个维的数据标准化到均值为0，方差为1的正太分布。即，使得<span class="math inline">\(\hat{x}^{(k)} \scriptsize{\sim} N(0, 1)\)</span></p>
<h3 id="bn的作用">5.4.1. BN的作用</h3>
<p>Batch Normalizaiton作用基本同Xavier初始化，不同的是，BN层不是从初始化考虑的，而是更直接在激励层输入之前强制标准化成固定均值及方差的高斯分布中。</p>
<p>整体来说有以下特点：<br>
1，对初始化权重参数没有要求 2，允许训练过程以高学习率来训练网络而不至于提督消失或剃度爆炸 3，可以提高网络的泛化能力，降低对dropout的依赖</p>
<h3 id="bn基本思想">5.4.2. BN基本思想</h3>
<p>1，将每一个维度的数据分布标准化到均值为0，方差为1的高斯分布中去<br>
2，针对强行把数据分布拉到高斯分布的数据再进行线形变换，补偿一部分标准化过程中数据压缩。大概思维同卷积神经网络的卷积和池化层之后接全链接层</p>
<h3 id="bn层的训练过程中的正向传播">5.4.3. BN层的训练过程中的正向传播</h3>
<p>使用m个样本更新参数时，m个样本的的某一个维度为例，下式中<span class="math inline">\(x_i\)</span>代表第<span class="math inline">\(i\)</span>个样本的第<span class="math inline">\(k\)</span>个分量。<span class="math inline">\(\beta\)</span>，<span class="math inline">\(\gamma\)</span>为未知参数，每层每个分量的共享，需要通过训练。<span class="math inline">\(\mu\)</span>，<span class="math inline">\(\sigma^2\)</span>:是由参与更新的的这个batch决定的</p>
<img src="/pic/ml/dnn/bn_transform.png" border="0" width="50%" height="50%" style="margin: 0 auto">
<center>
<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="external">BN训练过程中的正向传播</a>
</center>
<h3 id="bn层的使用">5.4.4. BN层的使用</h3>
<p>使用过程同正向传播，只是针对输入样本是其均值和方差由多个mini-batch的均值和方差的期望组成，既有N次mini-batch时：</p>
<p><span class="math display">\[
\mu = E_B[\mu_{B}]  \tag{24}
\]</span></p>
<p><span class="math display">\[
\sigma ^ 2 = \frac{m}{m-1}E_B[\sigma_{B} ^ 2]  \tag{25}
\]</span></p>
<p><span class="math display">\[
\hat{x_i} = \frac{x_i - \mu}{\sqrt{\sigma ^ 2 + \epsilon}}  \tag{26}
\]</span></p>
<p><span class="math display">\[
y_i = \gamma \hat{x_i} + \beta  \tag{27}
\]</span></p>
<p>其中<span class="math inline">\(\epsilon\)</span>为较小的数，防止分母为0的情况发生</p>
<h3 id="含有bn层的bp传播">5.4.5. 含有BN层的BP传播</h3>
<img src="/pic/ml/dnn/bn_gradient.png" border="0" width="60%" height="60%" style="margin: 0 auto">
<center>
<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="external">BN层的导数</a>
</center>
<p>则根据式（13） <span class="math display">\[
\begin{aligned}
&amp; \frac{\partial{\ell}}{\partial y_{i\ ,\ j_{i}}} 
    = \sum_{j_{i+1}\ \ = \ 1}^{J_{i+1}} \ 
        f^\prime \big(V_{IN_{i+1\ ,\ j_{i+1}}} \ \big) \ 
        \cdot \ 
        W_{i\ ,\ j_{i} \ \ , \ j_{i+1}} \ 
        \cdot \
        \frac{\partial{\ell}}{\partial O_{i+1\ ,\ j_{i+1}}}
\end{aligned}
\tag{28}
\]</span></p>
<p>则根据BN层的导数计算公式容易得到：</p>
<p><span class="math display">\[
\frac{\partial{\ell}}{\partial x_{i\ ,\ j_{i}}}   \tag{29}
\]</span></p>
<p><span class="math display">\[
\frac{\partial{\ell}}{\partial  \gamma_{i\ ,\ j_{i}}}  \tag{30} 
\]</span></p>
<p><span class="math display">\[
\frac{\partial{\ell}}{\partial \beta_{i\ ,\ j_{i}}}  \tag{31}
\]</span></p>
<p>为了和式（13）保持一致，更新式（13）为 <span class="math display">\[
\frac{\partial{\ell}}{\partial O_{i\ ,\ j_{i}}}
= \frac{\partial{\ell}}{\partial x_{i\ ,\ j_{i}}}   \tag{32}
\]</span> 上式表示，损失对经过激励函数输出的剃度，至此含有BN层和没有BN层的反向传播从形式上得到统一</p>
<h2 id="dropout">5.5. Dropout</h2>
<p><a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf" class="uri" target="_blank" rel="external">http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf</a></p>
<p>控制神经网络过拟合的一种方法</p>
<img src="/pic/ml/dnn/dnn_dropout.png" border="0" width="80%" height="80%" style="margin: 0 auto">
<center>
<a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf" target="_blank" rel="external">图2 dropout示意图</a>
</center>
<h3 id="基本思想">5.5.1. 基本思想</h3>
<p>有性繁殖和无性繁殖的区别： 无性生殖因为其遗传的单一性，总能容易的把大段优秀的基因遗传给子代，而有性生殖却不太具备把大段优秀的基因传给下一代。但是有性繁殖是最先进的生物进化方式</p>
<p>论文中提出有性繁殖优于无性繁殖论可能原因， 长期自然进化选择协作能力强的基因的而不是个体，协作方式更加稳健。</p>
<p>另一种理解就是有性繁殖避免了整个基因库走向极端，而不同基因的组合方式将基因多使得样性更容易快速适应不确定的未来。</p>
<p>基于有性繁殖的被保留下来的特性，训练神经网络过程中，不是用所有的神经元，而是一部分神经元。所有有了Dropout<br>
即：每个mini—bath训练时都以一定概率<span class="math inline">\(p\)</span>随机打开若干神经元（不参与本次运算），多次训练即多种网络结构集成达到控制过拟合的目的<br>
当<span class="math inline">\(p=0.5\)</span>时熵达到最大，即网络结构数量的期望可以达到最大值，这应该也是论文中提到<span class="math inline">\(0.5\)</span>的原因</p>
<h4 id="多模型集成">5.5.1.1. 多模型集成</h4>
<p>每次训练时神经网络结构都不同， 最后结果是通过多种结构叠加而成， 从这个角度来说Dropout有<a href="http://reset.pub/2017/04/04/ensemble-learning/" target="_blank" rel="external">Bagging</a>的思想</p>
<h4 id="权值共享">5.5.1.2. 权值共享</h4>
<p>考虑针对多个网络结构单独训练参数带来的时间成本，Dropout使用了权值共享策略，即所有网络的权值是相应位置是同一个权值，只是当某些神经元关闭时，该神经元和下一层链接神经元之间的权值不参与更新。从多个模型参数有协同参与完成模型预测的角度来说， Dropout具有<a href="http://reset.pub/2017/04/04/ensemble-learning/" target="_blank" rel="external">Boosting</a>的思想</p>
<h3 id="算法基本流程">5.5.2. 算法基本流程</h3>
<img src="/pic/ml/dnn/dropout_standard_vs_drop.png" border="0" width="80%" height="80%" style="margin: 0 auto">
<center>
<a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf" target="_blank" rel="external">图2 正常网络和引入dropout网络示意图</a>
</center>
<p>如上图，引入Dropout的神经网络在训练阶段每个神经元之后多了一个开关阀， 即在一次参数更新时，不是所有的神经元被激活</p>
<h4 id="参数训练">5.5.2.1. 参数训练</h4>
<p>初始化所有链接的权重 针对没次训练： 每个神经元以概率<span class="math inline">\(p\)</span>的情况关闭，即以概率<span class="math inline">\(p\)</span>打开神经网络的连接， 构建网网络，以之前权重为初始值更新本次有效链接的权值</p>
<h4 id="模型预测">5.5.2.2. 模型预测</h4>
<p>所有神经元的权值乘以概率<span class="math inline">\(p\)</span>，整个网络以未加入dorpout之前的结构进行正向传播进行结果预测。乘以概率<span class="math inline">\(p\)</span>可以理解为，网络在训练过程中，每个神经元始终是以概率<span class="math inline">\(p\)</span>参与最后的预测， 所以预测时需要以概率<span class="math inline">\(p\)</span>来打开，从期望的角度来可以看作是以每个神经元都被打开但其最终作用降为原来的<span class="math inline">\(p\)</span>倍，既保证了目标一致性的前提下，又使得神经元均参与数据预测，达到提高网络泛化能力的目的</p>
<p>假设训练结果得到某链接的权值为<span class="math inline">\(w\)</span>，预测过程中该神经元的贡献为<span class="math inline">\(w \cdot x\)</span>， 使用过程中该链接对应的值为<span class="math inline">\(p \cdot w \cdot x\)</span>， 假设<span class="math inline">\(w = \frac{1}{p} \cdot w^\prime\)</span>。则训练过程可以为<span class="math inline">\(\frac{1}{p} \cdot w^\prime\)</span>，模型使用过程中<span class="math inline">\(w^\prime \cdot x\)</span>。这样可以在使用过程中保证在含有Dropout网络输出的一致性，而不用单独处理</p>
<p>[深度学习中 Batch Normalizat为什么效果好]<a href="https://www.zhihu.com/question/38102762" class="uri" target="_blank" rel="external">https://www.zhihu.com/question/38102762</a></p>
<h2 id="其他提高泛化的方法">5.6. 其他提高泛化的方法</h2>
<p>L0-norm<br>
L1-norm<br>
L2-norm<br>
max-norm<br>
提前终止</p>
<p><a href="www.baidu.com">adsf</a></p>
<h1 id="参考">6. 参考</h1>
<p>[1] 2016.11.11 Ian Goodfellow, Yoshua Bengio, Aaron Courville 《Deep Learning》<br>
[2] 2017.03.15《Deep Learning翻译》<a href="https://exacity.github.io/deeplearningbook-chinese/" class="uri" target="_blank" rel="external">https://exacity.github.io/deeplearningbook-chinese/</a> [3] 2015.03.02 Google Inc <a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="external">Batch Normalization</a><br>
[4] 2014, Hintorn,etc<a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf" target="_blank" rel="external">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a><br>
[5] <a href="https://en.wikipedia.org/wiki/Backpropagation" class="uri" target="_blank" rel="external">https://en.wikipedia.org/wiki/Backpropagation</a><br>
[6] <a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="external">CS231n Convolutional Neural Networks for Visual Recognition</a><br>
[7] 2010 Xavier Glorot， Yoshua Bengio Understanding the difficulty of training deep feedforward neural networks, <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" class="uri" target="_blank" rel="external">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a><br>
[8] <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers/initializers" target="_blank" rel="external">TensorFlow.contrib.layers.xavier_initializer</a><br>
[9] <a href="http://blog.csdn.net/app_12062011/article/details/57956920" target="_blank" rel="external">权重初始化Xavier</a><br>
[10] <a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html" target="_blank" rel="external">Must Know Tips/Tricks in Deep Neural Networks</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DNN/" rel="tag"># DNN</a>
          
            <a href="/tags/BP/" rel="tag"># BP</a>
          
            <a href="/tags/xavier/" rel="tag"># xavier</a>
          
            <a href="/tags/BN/" rel="tag"># BN</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/04/04/ensemble-learning/" rel="next" title="集成方法">
                <i class="fa fa-chevron-left"></i> 集成方法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/05/23/cnn/" rel="prev" title="卷积神经网络简介">
                卷积神经网络简介 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/assets/blogImg/zhanghu798.jpg"
               alt="张虎" />
          <p class="site-author-name" itemprop="name">张虎</p>
          <p class="site-description motion-element" itemprop="description">记录、交流、进步</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              友情链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.xinyao.pub/" title="李昕垚" target="_blank">李昕垚</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#人工神经网络介绍"><span class="nav-text">1. 人工神经网络介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dnn与传统机器学习方法对比"><span class="nav-text">2. DNN与传统机器学习方法对比</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dnn基本结构"><span class="nav-text">3. DNN基本结构</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#激励函数"><span class="nav-text">4. 激励函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#sigmoid函数"><span class="nav-text">4.1. Sigmoid函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tanh函数"><span class="nav-text">4.2. tanh函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#relu"><span class="nav-text">4.3. Relu</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#leaky-relu"><span class="nav-text">4.4. Leaky ReLU</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络训练方法"><span class="nav-text">5. 神经网络训练方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#参数训练流程"><span class="nav-text">5.1. 参数训练流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播算法"><span class="nav-text">5.2. 反向传播算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#剃度计算及bp推导"><span class="nav-text">5.2.1. 剃度计算及BP推导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用bp-mini-batch-sgd训练的流程"><span class="nav-text">5.2.2. 使用BP + Mini-batch SGD训练的流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播简单例子"><span class="nav-text">5.2.3. 反向传播简单例子</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xavier参数初始化"><span class="nav-text">5.3. Xavier参数初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#问什么要使用初始化"><span class="nav-text">5.3.1. 问什么要使用初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基本假设"><span class="nav-text">5.3.2. 基本假设</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方差公式"><span class="nav-text">5.3.3. 方差公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#目标函数"><span class="nav-text">5.3.4. 目标函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#保证正向传播方差一致"><span class="nav-text">5.3.4.1. 保证正向传播方差一致</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#保证反向传播的方差一致"><span class="nav-text">5.3.4.2. 保证反向传播的方差一致</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结论"><span class="nav-text">5.3.5. 结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特别说明"><span class="nav-text">5.3.6. 特别说明</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#batch-normalization"><span class="nav-text">5.4. Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#bn的作用"><span class="nav-text">5.4.1. BN的作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bn基本思想"><span class="nav-text">5.4.2. BN基本思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bn层的训练过程中的正向传播"><span class="nav-text">5.4.3. BN层的训练过程中的正向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bn层的使用"><span class="nav-text">5.4.4. BN层的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#含有bn层的bp传播"><span class="nav-text">5.4.5. 含有BN层的BP传播</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout"><span class="nav-text">5.5. Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本思想"><span class="nav-text">5.5.1. 基本思想</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#多模型集成"><span class="nav-text">5.5.1.1. 多模型集成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#权值共享"><span class="nav-text">5.5.1.2. 权值共享</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法基本流程"><span class="nav-text">5.5.2. 算法基本流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#参数训练"><span class="nav-text">5.5.2.1. 参数训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型预测"><span class="nav-text">5.5.2.2. 模型预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他提高泛化的方法"><span class="nav-text">5.6. 其他提高泛化的方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-text">6. 参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张虎</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'reset-pub';
      var disqus_identifier = '2017/05/19/dnn/';

      var disqus_title = "深度神经网络简介";


      function run_disqus_script(disqus_script) {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');

      
        var disqus_config = function () {
            this.page.url = disqus_url;
            this.page.identifier = disqus_identifier;
            this.page.title = disqus_title;
        };
        run_disqus_script('embed.js');
      

    </script>
  










  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("rq0juDBslCtl5aXg0AoWw1q5-gzGzoHsz", "Y03tdkiQrnrHBQzsjfjdL29V");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  


</body>
</html>
